{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "from torchtext.datasets import WikiText2, IMDB\n",
    "import spacy\n",
    "import re\n",
    "import html\n",
    "from torchtext import data, datasets\n",
    "from spacy.symbols import ORTH\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as V\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need segment our data into seperate words and puncuation. We use spacys english tokenizer to process our data into tokens and to throw away the miscellaneous and irelevant parts of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en  = spacy.load('en')\n",
    "def tokenizer(x):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize = tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we create our vocabulary from our tokens. This step creates a mapping from tokens to integers and a mapping from integers back to tokens, we also include a max vocabular size as words that aren't in the 60,000 most frequently occuring probably won't have much of an impact or enough training data, and best treated as unknown. We use the 300 dimensional fasttext vectors to represent our tokens. These word vectors were released in 2017 by facebook AI research. The word representations are learned by taking into account subword information and incorporates character n-grams into the skipgram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = WikiText2.splits(TEXT)\n",
    "TEXT.build_vocab(train, vectors = \"fasttext.en.300d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to create a dataloader for our datasets. This dataloader will create batch each time we need one. When we specify that batch size we divide our training set, validation set and test set, into 64 different pieces of equivalent length. Backpropagation through time is specifys how long of sequences we will look at at a time. So when we specify 64 we specify that we won't consider tokens more than 64 words back when making a decision about what the next word most likely is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=16,\n",
    "    bptt_len=30, # this is where we specify the sequence length\n",
    "    device = \"cuda\",\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we create our our language model.  To begin the model encodes the tokens based on the fasttext word vectors. The main part of the model is the LSTM. The LSTM is a type of recurrent neural network. A reccurent neural network sequentially processes the sentence. At each step it takes in a new input word, encoded from the words vectors, and the previous hidden state. It reuses the same weights at each time step. The LSTM is a type if recurrent neural network which allows long term dependency information to flow through an additional path. This path can keep track of depencieslike plurality or negation. Finally the output layer makes our predictions. This is a fully connected layer that makes a prediction for the next word based on the current hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp,\n",
    "                 nhid, nlayers, bsz,\n",
    "                 dropout=0.5):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.nhid, self.nlayers, self.bsz = nhid, nlayers, bsz\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid,ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.hidden = self.init_hidden(bsz)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    " \n",
    "    def forward(self, input):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, self.hidden = self.rnn(emb, self.hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
    " \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (torch.tensor(weight.new(self.nlayers, bsz, self.nhid).zero_().cuda()),\n",
    "                torch.tensor(weight.new(self.nlayers, bsz, self.nhid).zero_()).cuda())\n",
    "  \n",
    "    def reset_history(self):\n",
    "        self.hidden = tuple(torch.tensor(h.data) for h in self.hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (encoder): Embedding(28870, 300)\n",
       "  (rnn): LSTM(300, 200, num_layers=3, dropout=0.5)\n",
       "  (decoder): Linear(in_features=200, out_features=28870, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix = TEXT.vocab.vectors\n",
    "model = LanguageModel(weight_matrix.size(0),\n",
    "weight_matrix.size(1), 200, 3, 16)\n",
    "model.encoder.weight.data.copy_(weight_matrix)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we train our model. We calculate the error using negative log likelihood loss and we propogate the error of our predictions back to the each of the weights in our network. Updating the weights by the error times a small learning rate. If our loss on the validation set does not improve for more than 3 epochs we decay the learning rate by 10. Because of dropout, randomly throwing away connections in our network with probability p, we are able to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def train_epoch(epoch,criterion, optimizer, n_tokens):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_iter):\n",
    "        model.reset_history()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        #set_trace()\n",
    "        batch_loss = loss.item() * prediction.size(0) * prediction.size(1)\n",
    "        \n",
    "        batch_loss /= len(train.examples[0].text)\n",
    "        \n",
    "        epoch_loss += batch_loss\n",
    "    \n",
    "    val_loss = 0\n",
    "        \n",
    "    for index, batch in enumerate(tqdm(valid_iter)):\n",
    "        model.reset_history()\n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        batch_loss = loss.item() * prediction.size(0) * prediction.size(1)\n",
    "        batch_loss /= len(valid.examples[0].text) \n",
    "        \n",
    "        val_loss += batch_loss\n",
    "        \n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))\n",
    "    return val_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3, betas=(0.7,0.99))\n",
    "n_tokens = weight_matrix.size(0)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=0, verbose = True)\n",
    "for i in range(30):\n",
    "    val_losses = []\n",
    "    val_loss = train_epoch(i,criterion, optimizer, n_tokens)\n",
    "    val_losses.append(val_loss)\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss == min(val_losses):\n",
    "        save_path = 'lstm_min_loss.pt'\n",
    "        torch.save(model.state_dict(), save_path)\"\"\"\n",
    "        \n",
    "model.load_state_dict(torch.load('lstm_min_loss.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classifier uses the same structure as our language model except for the linear layer. Instead of output after each word. We only take the last output of the rnn. We run our linear layer on this last output to predict to probabilities, one for the positive imdb class and one for the negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp,\n",
    "                 nhid, nlayers, bsz, noutputs,\n",
    "                 dropout=0.6):\n",
    "        super(ClassifierModel, self).__init__()\n",
    "        self.nhid, self.nlayers, self.bsz = nhid, nlayers, bsz\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.linear = nn.Linear(nhid,noutputs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        bsz = input.size()[1]\n",
    "        if bsz != self.bsz:\n",
    "            self.bsz = bsz\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        self.hidden = (torch.tensor(emb.data.new(*(self.nlayers, self.bsz, self.nhid)).zero_()),\n",
    "                       torch.tensor(emb.data.new(*(self.nlayers, self.bsz, self.nhid)).zero_()))\n",
    "        output, _ = self.rnn(emb, self.hidden)\n",
    "        return self.linear(self.drop(output[-1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierModel(\n",
       "  (drop): Dropout(p=0.6)\n",
       "  (encoder): Embedding(28870, 300)\n",
       "  (rnn): LSTM(300, 200, num_layers=3, dropout=0.6)\n",
       "  (linear): Linear(in_features=200, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = ClassifierModel(weight_matrix.size(0),\n",
    "weight_matrix.size(1), 200, 3,batch_size,2)\n",
    "model2.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the weights from the pretrained language model, we make sure the state is in the new model and the dimensions match, if not we initialize these weights from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_state = model.state_dict()\n",
    "model2_state = model2.state_dict()\n",
    "pretrained_state = { k:v for k,v in model1_state.items() if k in model2_state and v.size() == model2_state[k].size() }\n",
    "model2_state.update(pretrained_state)\n",
    "model2.load_state_dict(model2_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same text object from earlier to split our dataset, so we have the same int to string and string to int mapping as the previous dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = data.LabelField(tokenize='spacy')\n",
    "train2, test2 = datasets.IMDB.splits(TEXT,LABEL)\n",
    "train_iter2, test_iter2 = data.BucketIterator.splits((train2, test2), device='cuda', batch_size=batch_size,shuffle=True)\n",
    "LABEL.build_vocab(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model2.parameters(),lr=1e-3)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = loss_function.cuda()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, verbose = True)\n",
    "def fit(epoch,model2,data_loader,phase='training'):\n",
    "    if phase == 'training':\n",
    "        model2.train()\n",
    "    if phase == 'validation':\n",
    "        model2.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    run_total = 0\n",
    "    for batch_idx , batch in enumerate(tqdm(data_loader)):\n",
    "        text , target = batch.text , batch.label\n",
    "        \n",
    "        if phase == 'training':\n",
    "            optimizer.zero_grad()\n",
    "        output = model2(text)\n",
    "        output = output.squeeze(1)\n",
    "        loss = loss_function(output,target)\n",
    "        preds = output.data.max(dim=1,keepdim=True)[1]\n",
    "        running_correct += (preds.squeeze() == target).float().sum()\n",
    "        run_total += len(target)\n",
    "        running_loss += loss.detach()\n",
    "        if phase == 'training':            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = 100. * float(running_correct)/float(run_total)\n",
    "    print(phase.capitalize())\n",
    "    print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch, loss, accuracy))\n",
    "    return loss,accuracy\n",
    "train_losses , train_accuracy = [],[]\n",
    "val_losses , val_accuracy = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77c842069f442bbb94bcebc79f22f11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d730a351544c4ea8771c3903ac987d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training\n",
      "Epoch: 0, Loss: 0.1745, Accuracy: 49.9680\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be2152a44c0f4851af593df4087d0e20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation\n",
      "Epoch: 0, Loss: 0.1739, Accuracy: 47.9400\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "927e8c7495e54960b16f6219e2461120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6250), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "for epoch in tqdm(range(30)):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch,model2,train_iter2,phase='training')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch,model2,test_iter2,phase='validation')\n",
    "    scheduler.step(val_epoch_loss)\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_losses\")\n",
    "print(train_losses)\n",
    "print(\"train_accuracy\")\n",
    "print(train_accuracy)\n",
    "print(\"val_losses\")\n",
    "print(val_losses)\n",
    "print(\"val_accuracy\")\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model without transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = ClassifierModel(weight_matrix.size(0),\n",
    "weight_matrix.size(1), 200, 3, batch_size,2,dropout = 0.6)\n",
    "model3.cuda()\n",
    "optimizer = optim.Adam(model3.parameters(),lr=1e-3)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=2, verbose = True)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = loss_function.cuda()\n",
    "train_losses2 , train_accuracy2 = [],[]\n",
    "val_losses2 , val_accuracy2 = [],[] \n",
    "\n",
    "for epoch in tqdm(range(30)):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch,model3,train_iter2,phase='training')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch,model3,test_iter2,phase='validation')\n",
    "    scheduler.step(val_epoch_loss)\n",
    "    train_losses2.append(epoch_loss)\n",
    "    train_accuracy2.append(epoch_accuracy)\n",
    "    val_losses2.append(val_epoch_loss)\n",
    "    val_accuracy2.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train_losses\")\n",
    "print(train_losses2)\n",
    "print(\"train_accuracy\")\n",
    "print(train_accuracy2)\n",
    "print(\"val_losses\")\n",
    "print(val_losses2)\n",
    "print(\"val_accuracy\")\n",
    "print(val_accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
