{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "from torchtext.datasets import WikiText2\n",
    "import spacy\n",
    "import re\n",
    "import html\n",
    "from torchtext import data\n",
    "from spacy.symbols import ORTH\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as V\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogscats  dogscats.zip\twikitext-103  wikitext-2  wikitext-2-v1.zip\r\n"
     ]
    }
   ],
   "source": [
    "! ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en  = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize = tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train, valid, test = WikiText2.splits(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.datasets.language_modeling.WikiText2 at 0x7fa22d66a780>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', '<eos>', ' ', '=', 'valkyria', 'chronicles', 'iii', '=', '<eos>', ' ']\n"
     ]
    }
   ],
   "source": [
    "for ex in train.examples:\n",
    "    print(ex.text[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors = \"fasttext.en.300d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchtext.data.field.Field at 0x7fa22d66a668>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=32,\n",
    "    bptt_len=30, # this is where we specify the sequence length\n",
    "    device = \"cuda\",\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 32])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "    print(batch.text.shape)\n",
    "    #print(\"text\")\n",
    "    #print(batch.text.data)\n",
    "    #print(\"target\")\n",
    "    #print(batch.target.data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   12,    13,    12,    15,  3875,  3895,   889,    15,    13,    12,\n",
       "           13,    12, 20060,    93,  3875,    98,    52,     6,     8,     7,\n",
       "         3895,    27,   789,    52, 28868,     3,  6216,     4,  3875,     5],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.text.data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   13,    12,    15,  3875,  3895,   889,    15,    13,    12,    13,\n",
       "           12, 20060,    93,  3875,    98,    52,     6,     8,     7,  3895,\n",
       "           27,   789,    52, 28868,     3,  6216,     4,  3875,     5,     2],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.target.data[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp,\n",
    "                 nhid, nlayers, bsz,\n",
    "                 dropout=0.5):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.nhid, self.nlayers, self.bsz = nhid, nlayers, bsz\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid,ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.hidden = self.init_hidden(bsz)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    " \n",
    "    def forward(self, input):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, self.hidden = self.rnn(emb, self.hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        decoded_viewed = decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
    "        return decoded_viewed\n",
    " \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (torch.tensor(weight.new(self.nlayers, bsz, self.nhid).zero_().cuda()),\n",
    "                torch.tensor(weight.new(self.nlayers, bsz, self.nhid).zero_()).cuda())\n",
    "  \n",
    "    def reset_history(self):\n",
    "        self.hidden = tuple(torch.tensor(v.data) for v in self.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (encoder): Embedding(28870, 300)\n",
       "  (rnn): LSTM(300, 200, dropout=0.5)\n",
       "  (decoder): Linear(in_features=200, out_features=28870, bias=True)\n",
       "  (softmax): Softmax()\n",
       ")"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix = TEXT.vocab.vectors\n",
    "model = LanguageModel(weight_matrix.size(0),\n",
    "weight_matrix.size(1), 200, 1, 32)\n",
    "model.encoder.weight.data.copy_(weight_matrix)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3, betas=(0.9,0.999))\n",
    "n_tokens = weight_matrix.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "def train_epoch(epoch):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_iter):\n",
    "        model.reset_history()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item() * prediction.size(0) * prediction.size(1)\n",
    "        \n",
    "        batch_loss /= len(train.examples[0].text)\n",
    "        \n",
    "        epoch_loss += batch_loss\n",
    "        \n",
    "\n",
    "    val_loss = 0\n",
    "    for batch in valid_iter:\n",
    "        model.reset_history()\n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        batch_loss = loss.item() * text.size(0)\n",
    "        batch_loss /= len(valid.examples[0].text) \n",
    "        val_loss += batch_loss\n",
    "        \n",
    "        \n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2330/2330 [01:10<00:00, 32.85it/s]\n",
      "  0%|          | 0/2330 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 10.2135, Validation Loss: 0.3191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2330/2330 [01:10<00:00, 32.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Training Loss: 10.2121, Validation Loss: 0.3191\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    train_epoch(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def perplexity(text, prob_of):\n",
    "    total_prob_log = 1\n",
    "    index = 0\n",
    "    for word in text:\n",
    "        total_prob_log += math.log(prob_of(word, index))\n",
    "        index += 1\n",
    "    total_prob = math.exp(total_prob_log)\n",
    "    print(total_prob)\n",
    "    return math.pow(total_prob, -1/len(text))\n",
    "\n",
    "def single_batch(words, length):\n",
    "    return [[word] for word in words[:length]]\n",
    "\n",
    "def prob_of_model(model, text, word, index):\n",
    "    result_all = model(torch.tensor(single_batch(text, index + 1)).cuda())\n",
    "    result_values = result_all[result_all.shape[0]-1,0,:];\n",
    "    result_softmax = nn.Softmax(0)(result_values)\n",
    "    result = result_softmax[word].item()\n",
    "    print(result)\n",
    "    return result\n",
    "    \n",
    "def model_perplexity(text, model):\n",
    "    model.reset_history()\n",
    "    model.hidden = model.init_hidden(1)\n",
    "    prob_of = lambda word, index: prob_of_model(model, text, word, index)\n",
    "    return perplexity(text, prob_of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.463833490968682e-05\n",
      "3.463659595581703e-05\n",
      "3.463595930952579e-05\n",
      "3.463595930952579e-05\n",
      "3.463597749941982e-05\n",
      "3.4635962947504595e-05\n",
      "4.693489483747126e-27\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24439.03341276804"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_perplexity([13, 12, 15, 3875, 3895, 889], model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perplexity(\"test test test\", lambda x, index: 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
