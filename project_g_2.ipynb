{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "from torchtext.datasets import WikiText2\n",
    "import spacy\n",
    "import re\n",
    "import html\n",
    "from torchtext import data\n",
    "from spacy.symbols import ORTH\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as V\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self, ntoken, ninp,\n",
    "                 nhid, nlayers, bsz,\n",
    "                 dropout=0.5):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.nhid, self.nlayers, self.bsz = nhid, nlayers, bsz\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid,ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.hidden = self.init_hidden(bsz)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    " \n",
    "    def forward(self, input):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, self.hidden = self.rnn(emb, self.hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        decoded_viewed = decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
    "        return decoded_viewed\n",
    " \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (torch.tensor(weight.new(self.nlayers, bsz, self.nhid).zero_().cuda()),\n",
    "                torch.tensor(weight.new(self.nlayers, bsz, self.nhid).zero_()).cuda())\n",
    "  \n",
    "    def reset_history(self):\n",
    "        self.hidden = tuple(torch.tensor(v.data) for v in self.hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def validate_model(model, valid_iter, criterion, n_tokens, use_tqdm = False):\n",
    "    val_loss = 0\n",
    "    if use_tqdm:\n",
    "        valid_iter = tqdm(valid_iter)\n",
    "    for batch in valid_iter:\n",
    "        model.reset_history()\n",
    "        \n",
    "        text, targets = batch.text, batch.target\n",
    "        \n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        \n",
    "        batch_loss = loss.item() * text.size(0) / len(valid.examples[0].text)\n",
    "        val_loss += batch_loss\n",
    "    return val_loss\n",
    "    \n",
    "def train_model(model, train_iter, criterion, optimizer, n_tokens, use_tqdm = False):\n",
    "    epoch_loss = 0\n",
    "    if use_tqdm:\n",
    "        train_iter = tqdm(train_iter)\n",
    "    for batch in train_iter:\n",
    "        model.reset_history()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, targets = batch.text, batch.target\n",
    "        \n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_loss = loss.item() * prediction.size(0) * prediction.size(1) / len(train.examples[0].text)\n",
    "        epoch_loss += batch_loss\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def perplexity(text, prob_of):\n",
    "    total_prob_log = 1\n",
    "    index = 0\n",
    "    for word in text:\n",
    "        total_prob_log += math.log(prob_of(word, index))\n",
    "        index += 1\n",
    "    total_prob = math.exp(total_prob_log)\n",
    "    try:\n",
    "        return math.pow(total_prob, -1/len(text))\n",
    "    except:\n",
    "        return math.inf\n",
    "\n",
    "def single_batch(words, length):\n",
    "    return [[word] for word in words[:length]]\n",
    "\n",
    "def prob_of_model(model, text, word, index):\n",
    "    result_all = model(torch.tensor(single_batch(text, index + 1)).cuda())\n",
    "    result_values = result_all[result_all.shape[0]-1,0,:];\n",
    "    result_softmax = nn.Softmax(0)(result_values)\n",
    "    result = result_softmax[word].item()\n",
    "    return result\n",
    "    \n",
    "def model_perplexity(text, target, model):\n",
    "    model.reset_history()\n",
    "    model.hidden = model.init_hidden(1)\n",
    "    prob_of = lambda word, index: prob_of_model(model, text, word, index)\n",
    "    return perplexity(target, prob_of)\n",
    "\n",
    "def model_perplexity_batch(model, test_iter, use_tqdm = False):\n",
    "    if use_tqdm:\n",
    "        test_iter = tqdm(test_iter)\n",
    "    total_perplexity = 0\n",
    "    counts = 0\n",
    "    for batch in test_iter:\n",
    "        model.reset_history()\n",
    "        \n",
    "        texts, targets = batch.text, batch.target\n",
    "        results = model(texts)\n",
    "        for i in range(model.bsz): #32, second index in results\n",
    "            text = texts[:,i]\n",
    "            target = targets[:,i]\n",
    "            result = nn.Softmax(1)(results[:,i,:])\n",
    "            prob_of = lambda word, index: result[index][word]\n",
    "            \n",
    "            text_perplexity = perplexity(target, prob_of)\n",
    "            total_perplexity += text_perplexity\n",
    "            counts += 1\n",
    "            \n",
    "    \n",
    "    return total_perplexity / counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_training(epochs, use_tqdm = False):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-3, betas=(0.9,0.999))\n",
    "    n_tokens = weight_matrix.size(0)\n",
    "    print(\"Running...\")\n",
    "    for epoch in range(epochs):\n",
    "        #reset size to work with testing batch sizes\n",
    "        model.hidden = model.init_hidden(32)\n",
    "        \n",
    "        print(\" Epoch {}/{}\".format(epoch+1, epochs))\n",
    "        \n",
    "        print(\"   Running pre-validation\")\n",
    "        pre_val_loss = validate_model(model, valid_iter, criterion, n_tokens, use_tqdm)\n",
    "        print(\"   Training\")\n",
    "        train_loss = train_model(model, train_iter, criterion, optimizer, n_tokens, use_tqdm)\n",
    "        print(\"   Running post-validation\")\n",
    "        post_val_loss = validate_model(model, valid_iter, criterion, n_tokens, use_tqdm)\n",
    "        print(\"   Calculating perplexity\")\n",
    "        perplexity = model_perplexity_batch(model, test_iter, use_tqdm)\n",
    "        \n",
    "        print(\"  Results {}/{}: Training Loss: {:.4f}, Validation Loss: {:.4f} -> {:.4f}, Perplexity: {:.4f}\".format(epoch+1, epochs, train_loss, pre_val_loss, post_val_loss, perplexity))\n",
    "    print(\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (encoder): Embedding(28870, 300)\n",
       "  (rnn): LSTM(300, 200, dropout=0.5)\n",
       "  (decoder): Linear(in_features=200, out_features=28870, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(x):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(x)]\n",
    "\n",
    "TEXT = data.Field(lower=True, tokenize = tokenizer)\n",
    "\n",
    "train, valid, test = WikiText2.splits(TEXT)\n",
    "\n",
    "TEXT.build_vocab(train, vectors = \"fasttext.en.300d\")\n",
    "\n",
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=32,\n",
    "    bptt_len=30, # this is where we specify the sequence length\n",
    "    device = \"cuda\",\n",
    "    repeat=False)\n",
    "\n",
    "weight_matrix = TEXT.vocab.vectors\n",
    "model = LanguageModel(weight_matrix.size(0),\n",
    "weight_matrix.size(1), 200, 1, 32)\n",
    "model.encoder.weight.data.copy_(weight_matrix)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running...\n",
      " Epoch 1/5\n",
      "   Running pre-validation\n",
      "   Training\n",
      "   Running post-validation\n",
      "   Calculating perplexity\n",
      "  Results 1/5: Training Loss: 5.8237, Validation Loss: 0.3211 -> 0.1577, Perplexity: 198.0116\n",
      " Epoch 2/5\n",
      "   Running pre-validation\n",
      "   Training\n",
      "   Running post-validation\n",
      "   Calculating perplexity\n",
      "  Results 2/5: Training Loss: 5.2071, Validation Loss: 0.1577 -> 0.1513, Perplexity: 161.4524\n",
      " Epoch 3/5\n",
      "   Running pre-validation\n",
      "   Training\n",
      "   Running post-validation\n",
      "   Calculating perplexity\n",
      "  Results 3/5: Training Loss: 4.9659, Validation Loss: 0.1514 -> 0.1482, Perplexity: 146.9086\n",
      " Epoch 4/5\n",
      "   Running pre-validation\n",
      "   Training\n",
      "   Running post-validation\n",
      "   Calculating perplexity\n",
      "  Results 4/5: Training Loss: 4.8137, Validation Loss: 0.1482 -> 0.1467, Perplexity: 140.6538\n",
      " Epoch 5/5\n",
      "   Running pre-validation\n",
      "   Training\n",
      "   Running post-validation\n",
      "   Calculating perplexity\n",
      "  Results 5/5: Training Loss: 4.7072, Validation Loss: 0.1465 -> 0.1459, Perplexity: 137.0490\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "do_training(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
