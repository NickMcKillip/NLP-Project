{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace\n",
    "from torchtext.datasets import WikiText2, IMDB\n",
    "import spacy\n",
    "import re\n",
    "import html\n",
    "from torchtext import data, datasets\n",
    "from spacy.symbols import ORTH\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as V\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogscats  dogscats.zip\twikitext-103  wikitext-2  wikitext-2-v1.zip\r\n"
     ]
    }
   ],
   "source": [
    "! ls data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_en  = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(x):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(lower=True, tokenize = tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = WikiText2.splits(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, vectors = \"fasttext.en.300d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = data.BPTTIterator.splits(\n",
    "    (train, valid, test),\n",
    "    batch_size=16,\n",
    "    bptt_len=30, # this is where we specify the sequence length\n",
    "    device = \"cuda\",\n",
    "    repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp,\n",
    "                 nhid, nlayers, bsz,\n",
    "                 dropout=0.5):\n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.nhid, self.nlayers, self.bsz = nhid, nlayers, bsz\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.decoder = nn.Linear(nhid,ntoken)\n",
    "\n",
    "        self.init_weights()\n",
    "        self.hidden = self.init_hidden(bsz)\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    " \n",
    "    def forward(self, input):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        output, self.hidden = self.rnn(emb, self.hidden)\n",
    "        output = self.drop(output)\n",
    "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return decoded.view(output.size(0), output.size(1), decoded.size(1))\n",
    " \n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (torch.tensor(weight.new(self.nlayers, bsz, self.nhid).zero_().cuda()),\n",
    "                torch.tensor(weight.new(self.nlayers, bsz, self.nhid).zero_()).cuda())\n",
    "  \n",
    "    def reset_history(self):\n",
    "        self.hidden = tuple(torch.tensor(h.data) for h in self.hidden)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LanguageModel(\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (encoder): Embedding(28870, 300)\n",
       "  (rnn): LSTM(300, 200, num_layers=3, dropout=0.5)\n",
       "  (decoder): Linear(in_features=200, out_features=28870, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight_matrix = TEXT.vocab.vectors\n",
    "model = LanguageModel(weight_matrix.size(0),\n",
    "weight_matrix.size(1), 200, 3, 16)\n",
    "model.encoder.weight.data.copy_(weight_matrix)\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('lstm_8.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 1e-3, betas=(0.7,0.99))\n",
    "n_tokens = weight_matrix.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def train_epoch(epoch):\n",
    "    epoch_loss = 0\n",
    "    for batch in tqdm(train_iter):\n",
    "        model.reset_history()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        #set_trace()\n",
    "        batch_loss = loss.item() * prediction.size(0) * prediction.size(1)\n",
    "        \n",
    "        batch_loss /= len(train.examples[0].text)\n",
    "        \n",
    "        epoch_loss += batch_loss\n",
    "    \n",
    "    val_loss = 0\n",
    "        \n",
    "    for index, batch in enumerate(tqdm(valid_iter)):\n",
    "        model.reset_history()\n",
    "        text, targets = batch.text, batch.target\n",
    "        prediction = model(text)\n",
    "        loss = criterion(prediction.view(-1, n_tokens), targets.view(-1))\n",
    "        batch_loss = loss.item() * prediction.size(0) * prediction.size(1)\n",
    "        batch_loss /= len(valid.examples[0].text) \n",
    "        \n",
    "        val_loss += batch_loss\n",
    "        \n",
    "    print('Epoch: {}, Training Loss: {:.4f}, Validation Loss: {:.4f}'.format(epoch, epoch_loss, val_loss))\n",
    "    return val_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=0)\n",
    "for i in range(30):\n",
    "    val_losses = []\n",
    "    val_loss = train_epoch(i)\n",
    "    val_losses.append(val_loss)\n",
    "    scheduler.step(val_loss)\n",
    "    if val_loss == min(val_losses):\n",
    "        save_path = 'lstm_' + str(i) +'.pt'\n",
    "        torch.save(model.state_dict(), save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, ntoken, ninp,\n",
    "                 nhid, nlayers, bsz, noutputs,\n",
    "                 dropout=0.5):\n",
    "        super(ClassifierModel, self).__init__()\n",
    "        self.nhid, self.nlayers, self.bsz = nhid, nlayers, bsz\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        self.rnn = nn.LSTM(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.linear = nn.Linear(nhid,noutputs)\n",
    "\n",
    "    def forward(self, input):\n",
    "        bsz = input.size()[1]\n",
    "        if bsz != self.bsz:\n",
    "            self.bsz = bsz\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        self.hidden = (torch.tensor(emb.data.new(*(self.nlayers, self.bsz, self.nhid)).zero_()),\n",
    "                       torch.tensor(emb.data.new(*(self.nlayers, self.bsz, self.nhid)).zero_()))\n",
    "        output, _ = self.rnn(emb, self.hidden)\n",
    "        return self.linear(output[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassifierModel(\n",
       "  (drop): Dropout(p=0.5)\n",
       "  (encoder): Embedding(28870, 300)\n",
       "  (rnn): LSTM(300, 200, num_layers=3, dropout=0.5)\n",
       "  (linear): Linear(in_features=200, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = ClassifierModel(weight_matrix.size(0),\n",
    "weight_matrix.size(1), 200, 3, 4,2)\n",
    "model2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_state = model.state_dict()\n",
    "model2_state = model2.state_dict()\n",
    "pretrained_state = { k:v for k,v in model1_state.items() if k in model2_state and v.size() == model2_state[k].size() }\n",
    "model2_state.update(pretrained_state)\n",
    "model2.load_state_dict(model2_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = data.LabelField(tokenize='spacy')\n",
    "train2, test2 = datasets.IMDB.splits(TEXT,LABEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter2, test_iter2 = data.BucketIterator.splits((train2, test2), device='cuda', batch_size=4,shuffle=True)\n",
    "LABEL.build_vocab(train2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model2.parameters(),lr=1e-3)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "loss_function = loss_function.cuda()\n",
    "def fit(epoch,model2,data_loader,phase='training'):\n",
    "    if phase == 'training':\n",
    "        model2.train()\n",
    "    if phase == 'validation':\n",
    "        model2.eval()\n",
    "    running_loss = 0.0\n",
    "    running_correct = 0\n",
    "    run_total = 0\n",
    "    for batch_idx , batch in enumerate(data_loader):\n",
    "        text , target = batch.text , batch.label\n",
    "        \n",
    "        if phase == 'training':\n",
    "            optimizer.zero_grad()\n",
    "        output = model2(text)\n",
    "        output = output.squeeze(1)\n",
    "        loss = loss_function(output,target)\n",
    "        preds = output.data.max(dim=1,keepdim=True)[1]\n",
    "        running_correct += (preds.squeeze() == target).float().sum()\n",
    "        run_total += len(target)\n",
    "        running_loss += loss.detach()\n",
    "        if phase == 'training':            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    loss = running_loss/len(data_loader.dataset)\n",
    "    accuracy = 100. * float(running_correct)/float(run_total)\n",
    "    print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch, loss, accuracy))\n",
    "    return loss,accuracy\n",
    "train_losses , train_accuracy = [],[]\n",
    "val_losses , val_accuracy = [],[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f4cc4f2a2b74f6da4f2bfb89bf68169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=30), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 0.1748, Accuracy: 49.6480\n",
      "Epoch: 0, Loss: 0.1733, Accuracy: 50.0000\n",
      "Epoch: 1, Loss: 0.1744, Accuracy: 50.2480\n",
      "Epoch: 1, Loss: 0.1738, Accuracy: 50.0000\n",
      "Epoch: 2, Loss: 0.1742, Accuracy: 50.2120\n",
      "Epoch: 2, Loss: 0.1734, Accuracy: 50.0000\n",
      "Epoch: 3, Loss: 0.1742, Accuracy: 49.9640\n",
      "Epoch: 3, Loss: 0.1735, Accuracy: 50.0000\n",
      "Epoch: 4, Loss: 0.1742, Accuracy: 49.5280\n",
      "Epoch: 4, Loss: 0.1742, Accuracy: 50.0000\n",
      "Epoch: 5, Loss: 0.1740, Accuracy: 50.1320\n",
      "Epoch: 5, Loss: 0.1744, Accuracy: 50.0000\n",
      "Epoch: 6, Loss: 0.1739, Accuracy: 50.5240\n",
      "Epoch: 6, Loss: 0.1750, Accuracy: 50.0000\n",
      "Epoch: 7, Loss: 0.1740, Accuracy: 49.7440\n",
      "Epoch: 7, Loss: 0.1733, Accuracy: 50.0000\n",
      "Epoch: 8, Loss: 0.1739, Accuracy: 49.5320\n",
      "Epoch: 8, Loss: 0.1733, Accuracy: 50.0000\n",
      "Epoch: 9, Loss: 0.1739, Accuracy: 49.7720\n",
      "Epoch: 9, Loss: 0.1742, Accuracy: 50.0000\n",
      "Epoch: 10, Loss: 0.1739, Accuracy: 50.3720\n",
      "Epoch: 10, Loss: 0.1733, Accuracy: 50.0000\n",
      "Epoch: 11, Loss: 0.1738, Accuracy: 49.6000\n",
      "Epoch: 11, Loss: 0.1736, Accuracy: 50.0000\n",
      "Epoch: 12, Loss: 0.1738, Accuracy: 49.7920\n",
      "Epoch: 12, Loss: 0.1736, Accuracy: 50.0000\n",
      "Epoch: 13, Loss: 0.1738, Accuracy: 50.2000\n",
      "Epoch: 13, Loss: 0.1733, Accuracy: 50.0000\n",
      "Epoch: 14, Loss: 0.1739, Accuracy: 49.8640\n",
      "Epoch: 14, Loss: 0.1735, Accuracy: 50.0000\n",
      "Epoch: 15, Loss: 0.1738, Accuracy: 50.3080\n",
      "Epoch: 15, Loss: 0.1739, Accuracy: 50.0000\n",
      "Epoch: 16, Loss: 0.1738, Accuracy: 50.2800\n",
      "Epoch: 16, Loss: 0.1735, Accuracy: 50.0000\n",
      "Epoch: 17, Loss: 0.1738, Accuracy: 49.9720\n",
      "Epoch: 17, Loss: 0.1734, Accuracy: 50.0000\n",
      "Epoch: 18, Loss: 0.1739, Accuracy: 49.7040\n",
      "Epoch: 18, Loss: 0.1734, Accuracy: 50.0000\n",
      "Epoch: 19, Loss: 0.1738, Accuracy: 50.0960\n",
      "Epoch: 19, Loss: 0.1742, Accuracy: 50.0000\n",
      "Epoch: 20, Loss: 0.1738, Accuracy: 50.5320\n",
      "Epoch: 20, Loss: 0.1733, Accuracy: 50.0000\n",
      "Epoch: 21, Loss: 0.1737, Accuracy: 49.8480\n",
      "Epoch: 21, Loss: 0.1733, Accuracy: 50.0000\n",
      "Epoch: 22, Loss: 0.1737, Accuracy: 49.8040\n",
      "Epoch: 22, Loss: 0.1735, Accuracy: 50.0000\n",
      "Epoch: 23, Loss: 0.1736, Accuracy: 50.1440\n",
      "Epoch: 23, Loss: 0.1733, Accuracy: 50.0000\n",
      "Epoch: 24, Loss: 0.1737, Accuracy: 49.6760\n",
      "Epoch: 24, Loss: 0.1733, Accuracy: 50.0000\n",
      "Epoch: 25, Loss: 0.1737, Accuracy: 49.6440\n",
      "Epoch: 25, Loss: 0.1733, Accuracy: 50.0000\n",
      "Epoch: 26, Loss: 0.1736, Accuracy: 50.3000\n",
      "Epoch: 26, Loss: 0.1742, Accuracy: 50.0000\n",
      "Epoch: 27, Loss: 0.1736, Accuracy: 49.6440\n",
      "Epoch: 27, Loss: 0.1734, Accuracy: 50.0000\n",
      "Epoch: 28, Loss: 0.1736, Accuracy: 50.5280\n",
      "Epoch: 28, Loss: 0.1735, Accuracy: 50.0000\n",
      "Epoch: 29, Loss: 0.1737, Accuracy: 49.9680\n",
      "Epoch: 29, Loss: 0.1748, Accuracy: 50.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "for epoch in tqdm(range(30)):\n",
    "    epoch_loss, epoch_accuracy = fit(epoch,model2,train_iter2,phase='training')\n",
    "    val_epoch_loss , val_epoch_accuracy = fit(epoch,model2,test_iter2,phase='validation')\n",
    "    train_losses.append(epoch_loss)\n",
    "    train_accuracy.append(epoch_accuracy)\n",
    "    val_losses.append(val_epoch_loss)\n",
    "    val_accuracy.append(val_epoch_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_losses\n",
      "[tensor(0.1748, device='cuda:0'), tensor(0.1744, device='cuda:0'), tensor(0.1742, device='cuda:0'), tensor(0.1742, device='cuda:0'), tensor(0.1742, device='cuda:0'), tensor(0.1740, device='cuda:0'), tensor(0.1739, device='cuda:0'), tensor(0.1740, device='cuda:0'), tensor(0.1739, device='cuda:0'), tensor(0.1739, device='cuda:0'), tensor(0.1739, device='cuda:0'), tensor(0.1738, device='cuda:0'), tensor(0.1738, device='cuda:0'), tensor(0.1738, device='cuda:0'), tensor(0.1739, device='cuda:0'), tensor(0.1738, device='cuda:0'), tensor(0.1738, device='cuda:0'), tensor(0.1738, device='cuda:0'), tensor(0.1739, device='cuda:0'), tensor(0.1738, device='cuda:0'), tensor(0.1738, device='cuda:0'), tensor(0.1737, device='cuda:0'), tensor(0.1737, device='cuda:0'), tensor(0.1736, device='cuda:0'), tensor(0.1737, device='cuda:0'), tensor(0.1737, device='cuda:0'), tensor(0.1736, device='cuda:0'), tensor(0.1736, device='cuda:0'), tensor(0.1736, device='cuda:0'), tensor(0.1737, device='cuda:0')]\n",
      "train_accuracy\n",
      "[49.648, 50.248, 50.212, 49.964, 49.528, 50.132, 50.524, 49.744, 49.532, 49.772, 50.372, 49.6, 49.792, 50.2, 49.864, 50.308, 50.28, 49.972, 49.704, 50.096, 50.532, 49.848, 49.804, 50.144, 49.676, 49.644, 50.3, 49.644, 50.528, 49.968]\n",
      "val_losses\n",
      "[tensor(0.1733, device='cuda:0'), tensor(0.1738, device='cuda:0'), tensor(0.1734, device='cuda:0'), tensor(0.1735, device='cuda:0'), tensor(0.1742, device='cuda:0'), tensor(0.1744, device='cuda:0'), tensor(0.1750, device='cuda:0'), tensor(0.1733, device='cuda:0'), tensor(0.1733, device='cuda:0'), tensor(0.1742, device='cuda:0'), tensor(0.1733, device='cuda:0'), tensor(0.1736, device='cuda:0'), tensor(0.1736, device='cuda:0'), tensor(0.1733, device='cuda:0'), tensor(0.1735, device='cuda:0'), tensor(0.1739, device='cuda:0'), tensor(0.1735, device='cuda:0'), tensor(0.1734, device='cuda:0'), tensor(0.1734, device='cuda:0'), tensor(0.1742, device='cuda:0'), tensor(0.1733, device='cuda:0'), tensor(0.1733, device='cuda:0'), tensor(0.1735, device='cuda:0'), tensor(0.1733, device='cuda:0'), tensor(0.1733, device='cuda:0'), tensor(0.1733, device='cuda:0'), tensor(0.1742, device='cuda:0'), tensor(0.1734, device='cuda:0'), tensor(0.1735, device='cuda:0'), tensor(0.1748, device='cuda:0')]\n",
      "val_accuracy\n",
      "[50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0, 50.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"train_losses\")\n",
    "print(train_losses)\n",
    "print(\"train_accuracy\")\n",
    "print(train_accuracy)\n",
    "print(\"val_losses\")\n",
    "print(val_losses)\n",
    "print(\"val_accuracy\")\n",
    "print(val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0653, -0.0930, -0.0176,  ...,  0.1664, -0.1308,  0.0354],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0703, -0.1247, -0.2370,  ...,  0.1170, -0.0455,  0.1239]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0284, -0.0405, -0.0608,  ..., -0.0057, -0.0217, -0.0378],\n",
      "        [ 0.0334, -0.0247,  0.0382,  ...,  0.0177,  0.0546,  0.0211],\n",
      "        [-0.0496,  0.0235, -0.0351,  ..., -0.0190,  0.0028,  0.0683],\n",
      "        ...,\n",
      "        [-0.0429,  0.0583, -0.0559,  ...,  0.0653,  0.0500,  0.0285],\n",
      "        [ 0.0372,  0.0490,  0.0424,  ...,  0.0239, -0.0578, -0.0526],\n",
      "        [ 0.0456, -0.0687,  0.0115,  ..., -0.0327,  0.0559, -0.0433]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0186,  0.0189,  0.0461,  ..., -0.0227,  0.0083,  0.0664],\n",
      "        [ 0.0009, -0.0493, -0.0304,  ..., -0.0221, -0.0133, -0.0390],\n",
      "        [-0.0083, -0.0053,  0.0399,  ...,  0.0269, -0.0573, -0.0566],\n",
      "        ...,\n",
      "        [ 0.0211, -0.0353, -0.0528,  ...,  0.0569,  0.0214, -0.0343],\n",
      "        [-0.0605,  0.0341,  0.0374,  ...,  0.0593,  0.0210, -0.0675],\n",
      "        [-0.0403, -0.0292,  0.0084,  ..., -0.0624, -0.0171, -0.0481]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0004, -0.0696,  0.0249, -0.0176, -0.0112, -0.0057, -0.0157,  0.0250,\n",
      "        -0.0490,  0.0473,  0.0499,  0.0387,  0.0086,  0.0056, -0.0379,  0.0663,\n",
      "        -0.0139, -0.0384, -0.0244, -0.0029,  0.0653, -0.0375,  0.0158, -0.0112,\n",
      "        -0.0011, -0.0130, -0.0240,  0.0439,  0.0250,  0.0277, -0.0054, -0.0425,\n",
      "         0.0228,  0.0405, -0.0040, -0.0465,  0.0506, -0.0524, -0.0032, -0.0372,\n",
      "         0.0446,  0.0556,  0.0547, -0.0334, -0.0245, -0.0419,  0.0550,  0.0391,\n",
      "        -0.0487,  0.0530, -0.0472,  0.0365, -0.0328,  0.0595, -0.0543,  0.0361,\n",
      "         0.0004,  0.0383,  0.0515,  0.0490, -0.0578, -0.0246,  0.0319, -0.0022,\n",
      "        -0.0120, -0.0443, -0.0359, -0.0382, -0.0579,  0.0614,  0.0083, -0.0236,\n",
      "        -0.0227, -0.0237, -0.0601, -0.0384, -0.0400,  0.0385, -0.0331, -0.0402,\n",
      "         0.0174,  0.0010, -0.0196,  0.0617, -0.0079, -0.0502, -0.0643,  0.0060,\n",
      "        -0.0098,  0.0252, -0.0084, -0.0140, -0.0378,  0.0352, -0.0513,  0.0063,\n",
      "        -0.0342,  0.0550, -0.0532,  0.0410,  0.0280, -0.0643,  0.0056, -0.0587,\n",
      "         0.0234, -0.0577, -0.0288,  0.0032, -0.0380,  0.0121, -0.0300,  0.0467,\n",
      "        -0.0616, -0.0192, -0.0257,  0.0060,  0.0533,  0.0291, -0.0239,  0.0353,\n",
      "        -0.0254, -0.0091,  0.0276,  0.0049, -0.0538, -0.0047,  0.0342,  0.0094,\n",
      "         0.0251, -0.0235, -0.0287, -0.0511, -0.0216, -0.0390,  0.0557,  0.0300,\n",
      "         0.0290,  0.0475, -0.0609, -0.0084, -0.0164, -0.0392, -0.0386, -0.0540,\n",
      "        -0.0220,  0.0195,  0.0267,  0.0590, -0.0370,  0.0433,  0.0141, -0.0409,\n",
      "        -0.0577, -0.0233, -0.0143,  0.0617, -0.0270,  0.0545,  0.0340,  0.0501,\n",
      "         0.0290,  0.0173,  0.0631, -0.0565, -0.0104,  0.0248,  0.0591, -0.0084,\n",
      "        -0.0593, -0.0015, -0.0546,  0.0646, -0.0332, -0.0444, -0.0023,  0.0219,\n",
      "         0.0272,  0.0350,  0.0685,  0.0545, -0.0652,  0.0292,  0.0201, -0.0319,\n",
      "        -0.0364, -0.0403, -0.0506,  0.0128, -0.0441, -0.0451,  0.0694, -0.0009,\n",
      "        -0.0487,  0.0641,  0.0453, -0.0422, -0.0380,  0.0067,  0.0025, -0.0371,\n",
      "         0.0021, -0.0381,  0.0041,  0.0052,  0.0304, -0.0637,  0.0124,  0.0457,\n",
      "        -0.0347,  0.0449,  0.0037, -0.0502, -0.0389, -0.0661,  0.0401,  0.0501,\n",
      "         0.0373, -0.0477,  0.0386,  0.0647,  0.0264, -0.0063, -0.0386,  0.0137,\n",
      "        -0.0663,  0.0191,  0.0458, -0.0606, -0.0664, -0.0324,  0.0637,  0.0103,\n",
      "         0.0055, -0.0471, -0.0354,  0.0425, -0.0408, -0.0311, -0.0658,  0.0066,\n",
      "        -0.0450,  0.0694,  0.0512, -0.0354,  0.0463, -0.0105, -0.0483, -0.0596,\n",
      "        -0.0341,  0.0467,  0.0234, -0.0220,  0.0641, -0.0462, -0.0435, -0.0689,\n",
      "        -0.0040,  0.0580, -0.0566,  0.0392,  0.0465,  0.0003, -0.0393, -0.0041,\n",
      "        -0.0684, -0.0351, -0.0248,  0.0472, -0.0387,  0.0702,  0.0018,  0.0617,\n",
      "        -0.0252, -0.0630, -0.0296, -0.0606,  0.0618,  0.0519,  0.0505, -0.0497,\n",
      "        -0.0277, -0.0087, -0.0279, -0.0412,  0.0600,  0.0073, -0.0670,  0.0002,\n",
      "        -0.0678,  0.0514, -0.0633, -0.0656,  0.0420,  0.0642,  0.0153,  0.0679,\n",
      "        -0.0446, -0.0419, -0.0461,  0.0083,  0.0284,  0.0676, -0.0104, -0.0334,\n",
      "         0.0657,  0.0660,  0.0528,  0.0077, -0.0560, -0.0101,  0.0704,  0.0538,\n",
      "        -0.0031,  0.0100,  0.0621, -0.0707, -0.0664,  0.0074, -0.0213,  0.0105,\n",
      "         0.0323, -0.0543,  0.0052, -0.0304,  0.0672, -0.0404, -0.0540,  0.0455,\n",
      "         0.0220,  0.0170, -0.0656,  0.0135,  0.0390, -0.0250, -0.0486,  0.0418,\n",
      "        -0.0415, -0.0302,  0.0580, -0.0326, -0.0389, -0.0315, -0.0489, -0.0562,\n",
      "        -0.0623, -0.0458,  0.0668, -0.0053, -0.0631, -0.0009, -0.0339, -0.0129,\n",
      "        -0.0132,  0.0620, -0.0124, -0.0347,  0.0537,  0.0394,  0.0437, -0.0004,\n",
      "         0.0467,  0.0648,  0.0355,  0.0373,  0.0553,  0.0160,  0.0613,  0.0427,\n",
      "        -0.0098,  0.0683,  0.0198, -0.0701, -0.0348,  0.0678, -0.0662,  0.0004,\n",
      "        -0.0510,  0.0253, -0.0660,  0.0369, -0.0542,  0.0573,  0.0656,  0.0581,\n",
      "         0.0278,  0.0276,  0.0536, -0.0330, -0.0545,  0.0421, -0.0481,  0.0279,\n",
      "        -0.0332, -0.0439,  0.0207, -0.0397, -0.0318,  0.0346,  0.0102, -0.0323,\n",
      "        -0.0466, -0.0367, -0.0009,  0.0216,  0.0394, -0.0294, -0.0346, -0.0062,\n",
      "         0.0411,  0.0381,  0.0068, -0.0367,  0.0521, -0.0110, -0.0081, -0.0376,\n",
      "        -0.0285, -0.0622,  0.0589,  0.0435,  0.0646,  0.0098, -0.0375,  0.0005,\n",
      "        -0.0647, -0.0181, -0.0278, -0.0399, -0.0403,  0.0612,  0.0427,  0.0123,\n",
      "        -0.0045, -0.0549,  0.0022,  0.0234,  0.0244, -0.0540, -0.0021,  0.0362,\n",
      "        -0.0020,  0.0656,  0.0041,  0.0035,  0.0624, -0.0491, -0.0479, -0.0199,\n",
      "         0.0588, -0.0155, -0.0020,  0.0361, -0.0574,  0.0384,  0.0631,  0.0369,\n",
      "        -0.0155,  0.0202,  0.0593, -0.0433, -0.0444, -0.0017,  0.0006, -0.0154,\n",
      "        -0.0353,  0.0223, -0.0620, -0.0698,  0.0533, -0.0171,  0.0412, -0.0391,\n",
      "         0.0492,  0.0112,  0.0252, -0.0030,  0.0073, -0.0010, -0.0537, -0.0195,\n",
      "         0.0112,  0.0129, -0.0515,  0.0161, -0.0518, -0.0159,  0.0204,  0.0335,\n",
      "         0.0516, -0.0026,  0.0707,  0.0222,  0.0002, -0.0142,  0.0601,  0.0110,\n",
      "         0.0288, -0.0124,  0.0076,  0.0539, -0.0478,  0.0505, -0.0500,  0.0536,\n",
      "         0.0088,  0.0578, -0.0458, -0.0299,  0.0579, -0.0314, -0.0271,  0.0141,\n",
      "         0.0538,  0.0462,  0.0343,  0.0320, -0.0292, -0.0094, -0.0270, -0.0273,\n",
      "         0.0254, -0.0454,  0.0352,  0.0194,  0.0566, -0.0228,  0.0597,  0.0432,\n",
      "         0.0083,  0.0213,  0.0226,  0.0655, -0.0677, -0.0068,  0.0605,  0.0155,\n",
      "         0.0001, -0.0448, -0.0208,  0.0513, -0.0114,  0.0583,  0.0497, -0.0407,\n",
      "         0.0364, -0.0368, -0.0339, -0.0433,  0.0117, -0.0477, -0.0083,  0.0442,\n",
      "        -0.0150,  0.0449, -0.0454,  0.0539, -0.0136, -0.0051,  0.0158,  0.0247,\n",
      "        -0.0033,  0.0461,  0.0396,  0.0321,  0.0143,  0.0100,  0.0181,  0.0595,\n",
      "        -0.0662,  0.0296, -0.0064, -0.0194,  0.0038, -0.0052, -0.0218, -0.0534,\n",
      "         0.0254,  0.0060,  0.0621,  0.0406,  0.0147,  0.0360,  0.0355, -0.0459,\n",
      "         0.0125,  0.0139,  0.0307, -0.0168,  0.0457, -0.0119,  0.0479, -0.0083,\n",
      "        -0.0121, -0.0155,  0.0577, -0.0242,  0.0073, -0.0244,  0.0210, -0.0240,\n",
      "         0.0439, -0.0518,  0.0556,  0.0651, -0.0518,  0.0033, -0.0267,  0.0473,\n",
      "        -0.0097, -0.0160,  0.0018,  0.0294, -0.0451,  0.0374,  0.0074,  0.0611,\n",
      "        -0.0091,  0.0437,  0.0362,  0.0409,  0.0659,  0.0562, -0.0120, -0.0406,\n",
      "        -0.0497,  0.0343, -0.0045,  0.0438, -0.0420,  0.0176, -0.0122,  0.0407,\n",
      "        -0.0405,  0.0603,  0.0681, -0.0391, -0.0353, -0.0421,  0.0378,  0.0361,\n",
      "        -0.0080,  0.0082, -0.0606,  0.0706, -0.0064,  0.0247, -0.0399,  0.0513,\n",
      "         0.0680,  0.0105, -0.0120, -0.0658,  0.0615,  0.0220,  0.0707, -0.0652,\n",
      "        -0.0162, -0.0458,  0.0401,  0.0465, -0.0601, -0.0082, -0.0631,  0.0247,\n",
      "         0.0643,  0.0277, -0.0011, -0.0321,  0.0379,  0.0065, -0.0147,  0.0376,\n",
      "         0.0330,  0.0109,  0.0473, -0.0165,  0.0164, -0.0102,  0.0238, -0.0499,\n",
      "         0.0578,  0.0078,  0.0296, -0.0374,  0.0319, -0.0272,  0.0432, -0.0477,\n",
      "        -0.0648, -0.0666, -0.0587,  0.0085,  0.0122, -0.0535, -0.0122, -0.0481,\n",
      "         0.0693, -0.0646,  0.0543, -0.0508,  0.0616, -0.0512, -0.0011, -0.0364,\n",
      "        -0.0011, -0.0501, -0.0379,  0.0075, -0.0428, -0.0123,  0.0361, -0.0255,\n",
      "        -0.0288, -0.0267, -0.0046,  0.0539,  0.0014, -0.0274,  0.0143,  0.0548,\n",
      "        -0.0657, -0.0115, -0.0310,  0.0399, -0.0305, -0.0590,  0.0168,  0.0291,\n",
      "        -0.0592, -0.0083,  0.0175,  0.0645, -0.0529, -0.0553, -0.0498, -0.0581,\n",
      "         0.0186, -0.0423, -0.0683,  0.0287,  0.0501,  0.0529, -0.0562, -0.0154,\n",
      "         0.0480, -0.0425,  0.0706, -0.0387, -0.0437,  0.0255,  0.0676, -0.0060,\n",
      "         0.0529,  0.0361,  0.0498,  0.0340, -0.0081,  0.0657,  0.0355, -0.0481,\n",
      "         0.0665, -0.0592, -0.0047,  0.0090, -0.0347,  0.0036, -0.0703,  0.0595,\n",
      "         0.0336,  0.0143,  0.0642,  0.0400,  0.0014,  0.0157, -0.0670, -0.0413,\n",
      "        -0.0258,  0.0310, -0.0079,  0.0067,  0.0465,  0.0511, -0.0424,  0.0108,\n",
      "        -0.0311, -0.0393, -0.0228, -0.0086, -0.0278,  0.0131,  0.0364, -0.0301,\n",
      "         0.0468, -0.0434, -0.0438, -0.0498,  0.0685, -0.0404,  0.0466, -0.0250],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0321,  0.0038,  0.0446, -0.0495, -0.0422, -0.0393, -0.0503,  0.0169,\n",
      "        -0.0038, -0.0305, -0.0215,  0.0249,  0.0520,  0.0423,  0.0670, -0.0277,\n",
      "         0.0043, -0.0083, -0.0277, -0.0146, -0.0094, -0.0231, -0.0677,  0.0349,\n",
      "         0.0528, -0.0640, -0.0603, -0.0432, -0.0633, -0.0446,  0.0570, -0.0313,\n",
      "         0.0632, -0.0292,  0.0402, -0.0160, -0.0203, -0.0273, -0.0216, -0.0536,\n",
      "        -0.0550,  0.0533, -0.0442,  0.0274,  0.0044,  0.0428, -0.0084, -0.0476,\n",
      "        -0.0330, -0.0050, -0.0394, -0.0558, -0.0700,  0.0294,  0.0209, -0.0150,\n",
      "        -0.0193, -0.0427,  0.0441,  0.0423, -0.0659,  0.0252,  0.0706,  0.0124,\n",
      "        -0.0024,  0.0673,  0.0568, -0.0004, -0.0139,  0.0592, -0.0310, -0.0233,\n",
      "         0.0634, -0.0116,  0.0154, -0.0323,  0.0641, -0.0575, -0.0062,  0.0177,\n",
      "        -0.0455,  0.0522,  0.0400, -0.0674,  0.0074,  0.0449,  0.0162,  0.0090,\n",
      "        -0.0372, -0.0603, -0.0242, -0.0547,  0.0355,  0.0291,  0.0480, -0.0474,\n",
      "        -0.0599, -0.0613, -0.0637,  0.0473, -0.0257, -0.0520,  0.0294, -0.0175,\n",
      "         0.0689,  0.0168,  0.0595,  0.0698,  0.0346,  0.0343,  0.0497,  0.0005,\n",
      "         0.0196,  0.0631,  0.0580,  0.0289, -0.0276, -0.0212,  0.0666, -0.0315,\n",
      "         0.0461, -0.0554,  0.0561,  0.0527,  0.0697,  0.0386,  0.0483, -0.0514,\n",
      "        -0.0094,  0.0543, -0.0335,  0.0318, -0.0494,  0.0338,  0.0229,  0.0093,\n",
      "         0.0557,  0.0083,  0.0442,  0.0583, -0.0094,  0.0368,  0.0440,  0.0497,\n",
      "        -0.0404,  0.0495,  0.0111,  0.0642,  0.0113, -0.0029, -0.0113, -0.0391,\n",
      "         0.0295, -0.0040,  0.0133, -0.0010,  0.0261,  0.0603,  0.0087,  0.0226,\n",
      "         0.0342,  0.0646, -0.0669, -0.0434,  0.0303,  0.0406,  0.0073, -0.0524,\n",
      "        -0.0094, -0.0016, -0.0089, -0.0403,  0.0356, -0.0385,  0.0676,  0.0349,\n",
      "        -0.0332,  0.0376,  0.0140,  0.0064,  0.0270, -0.0043, -0.0399, -0.0338,\n",
      "        -0.0366, -0.0152,  0.0023,  0.0421, -0.0472,  0.0279,  0.0656,  0.0209,\n",
      "         0.0413, -0.0388,  0.0124,  0.0192, -0.0140, -0.0041,  0.0360,  0.0324,\n",
      "         0.0302,  0.0163, -0.0323,  0.0001,  0.0403,  0.0171,  0.0564, -0.0395,\n",
      "         0.0133, -0.0081,  0.0442, -0.0173,  0.0388,  0.0449,  0.0031,  0.0213,\n",
      "        -0.0487, -0.0220, -0.0639,  0.0111,  0.0444, -0.0593,  0.0106,  0.0677,\n",
      "         0.0084,  0.0372, -0.0311,  0.0535, -0.0513,  0.0225,  0.0357,  0.0700,\n",
      "        -0.0114,  0.0012,  0.0439, -0.0348, -0.0209,  0.0494, -0.0646, -0.0419,\n",
      "         0.0178, -0.0235, -0.0674,  0.0306,  0.0192, -0.0476, -0.0416, -0.0176,\n",
      "         0.0409, -0.0213,  0.0273, -0.0585, -0.0559, -0.0252, -0.0048,  0.0063,\n",
      "         0.0652,  0.0475,  0.0408, -0.0053, -0.0017, -0.0433,  0.0374,  0.0014,\n",
      "         0.0255,  0.0634, -0.0684, -0.0623,  0.0252,  0.0316, -0.0338, -0.0399,\n",
      "        -0.0593, -0.0075,  0.0667,  0.0682, -0.0251, -0.0455, -0.0347,  0.0575,\n",
      "         0.0157,  0.0410,  0.0312,  0.0179, -0.0704, -0.0400, -0.0253,  0.0207,\n",
      "         0.0103,  0.0379, -0.0101, -0.0539, -0.0681,  0.0507,  0.0394, -0.0331,\n",
      "        -0.0276,  0.0317, -0.0504,  0.0416, -0.0286,  0.0465,  0.0551, -0.0684,\n",
      "         0.0497,  0.0445,  0.0426, -0.0691,  0.0049,  0.0035, -0.0159, -0.0614,\n",
      "         0.0478, -0.0061,  0.0396,  0.0432,  0.0634, -0.0081, -0.0032,  0.0645,\n",
      "         0.0527, -0.0086,  0.0047, -0.0557,  0.0485,  0.0397, -0.0200, -0.0484,\n",
      "         0.0262,  0.0044, -0.0146,  0.0428,  0.0156,  0.0523, -0.0307, -0.0228,\n",
      "        -0.0541,  0.0631,  0.0014,  0.0337,  0.0258, -0.0238,  0.0524,  0.0386,\n",
      "        -0.0225, -0.0582,  0.0167,  0.0067,  0.0548,  0.0228,  0.0505,  0.0024,\n",
      "         0.0620, -0.0347, -0.0305, -0.0192,  0.0524,  0.0653,  0.0050, -0.0704,\n",
      "         0.0102,  0.0133,  0.0218, -0.0215,  0.0554,  0.0665,  0.0279, -0.0230,\n",
      "        -0.0432, -0.0291, -0.0685,  0.0639, -0.0310, -0.0388,  0.0061,  0.0521,\n",
      "        -0.0442, -0.0288, -0.0585, -0.0569, -0.0022, -0.0556,  0.0102, -0.0065,\n",
      "        -0.0146,  0.0449, -0.0610, -0.0406,  0.0629, -0.0625,  0.0260,  0.0514,\n",
      "        -0.0531,  0.0597, -0.0244,  0.0268,  0.0271,  0.0485, -0.0610,  0.0413,\n",
      "        -0.0081,  0.0593,  0.0233, -0.0593, -0.0090, -0.0137, -0.0604, -0.0651,\n",
      "        -0.0699, -0.0571, -0.0235,  0.0185,  0.0276,  0.0665,  0.0458, -0.0662,\n",
      "         0.0417,  0.0555,  0.0550, -0.0638,  0.0063,  0.0516, -0.0223,  0.0623,\n",
      "        -0.0035, -0.0616, -0.0209,  0.0008, -0.0288,  0.0390, -0.0368, -0.0113,\n",
      "        -0.0188,  0.0595,  0.0233,  0.0275, -0.0025,  0.0264, -0.0175,  0.0693,\n",
      "         0.0245, -0.0185, -0.0630,  0.0636, -0.0031, -0.0336, -0.0403, -0.0201,\n",
      "        -0.0465,  0.0078,  0.0644,  0.0176, -0.0586, -0.0388,  0.0157,  0.0621,\n",
      "        -0.0283, -0.0698, -0.0373,  0.0390,  0.0693, -0.0318, -0.0213,  0.0475,\n",
      "         0.0062,  0.0375,  0.0488,  0.0060, -0.0098, -0.0160,  0.0541, -0.0513,\n",
      "         0.0397,  0.0549, -0.0512,  0.0564,  0.0316,  0.0569, -0.0094,  0.0061,\n",
      "         0.0543,  0.0423, -0.0214,  0.0566,  0.0324, -0.0659,  0.0244, -0.0457,\n",
      "        -0.0251, -0.0241, -0.0261, -0.0251, -0.0526, -0.0291,  0.0661, -0.0263,\n",
      "         0.0658,  0.0133, -0.0274,  0.0114, -0.0633,  0.0406,  0.0407,  0.0083,\n",
      "         0.0521, -0.0257, -0.0670,  0.0554,  0.0115, -0.0477,  0.0086, -0.0644,\n",
      "         0.0150, -0.0559,  0.0017,  0.0050, -0.0513,  0.0372, -0.0692,  0.0538,\n",
      "         0.0044, -0.0501,  0.0508, -0.0483,  0.0659,  0.0356,  0.0488, -0.0612,\n",
      "         0.0276, -0.0236,  0.0320,  0.0063, -0.0688,  0.0488, -0.0344, -0.0033,\n",
      "         0.0373,  0.0490, -0.0047, -0.0533,  0.0647, -0.0409,  0.0051, -0.0203,\n",
      "        -0.0574,  0.0251,  0.0171, -0.0510, -0.0513, -0.0338, -0.0557, -0.0270,\n",
      "         0.0584,  0.0488, -0.0697,  0.0404, -0.0691,  0.0565, -0.0629, -0.0023,\n",
      "         0.0251,  0.0242,  0.0557,  0.0561,  0.0512, -0.0074, -0.0430, -0.0695,\n",
      "        -0.0534, -0.0662, -0.0116,  0.0193,  0.0169, -0.0143,  0.0275,  0.0470,\n",
      "         0.0155,  0.0539,  0.0569, -0.0206,  0.0491,  0.0326,  0.0494, -0.0273,\n",
      "         0.0499, -0.0402,  0.0617, -0.0417, -0.0369, -0.0497,  0.0459, -0.0129,\n",
      "         0.0437, -0.0205,  0.0043,  0.0481,  0.0127, -0.0140,  0.0599, -0.0137,\n",
      "         0.0600,  0.0113,  0.0054, -0.0158,  0.0269, -0.0250, -0.0611,  0.0178,\n",
      "         0.0129,  0.0611, -0.0265, -0.0230, -0.0547,  0.0324,  0.0568, -0.0210,\n",
      "        -0.0404,  0.0060,  0.0008,  0.0562,  0.0402, -0.0626,  0.0442,  0.0168,\n",
      "        -0.0383, -0.0584,  0.0507, -0.0575,  0.0472,  0.0197, -0.0590, -0.0375,\n",
      "        -0.0471,  0.0266,  0.0312,  0.0696,  0.0066, -0.0686, -0.0628,  0.0095,\n",
      "         0.0228, -0.0378, -0.0592, -0.0250, -0.0515,  0.0594,  0.0585, -0.0132,\n",
      "        -0.0294,  0.0344,  0.0707, -0.0446,  0.0363, -0.0692,  0.0169,  0.0656,\n",
      "        -0.0301,  0.0090, -0.0004,  0.0271,  0.0268, -0.0427,  0.0573,  0.0197,\n",
      "        -0.0094, -0.0700,  0.0181,  0.0125, -0.0499,  0.0173,  0.0171,  0.0537,\n",
      "         0.0025,  0.0505,  0.0121, -0.0216, -0.0696, -0.0195, -0.0668,  0.0111,\n",
      "         0.0021, -0.0184,  0.0253,  0.0614, -0.0678,  0.0636, -0.0054,  0.0702,\n",
      "        -0.0313, -0.0259,  0.0398,  0.0316,  0.0033,  0.0373, -0.0153, -0.0019,\n",
      "        -0.0657, -0.0115, -0.0021, -0.0100,  0.0490, -0.0161,  0.0242,  0.0199,\n",
      "        -0.0510,  0.0059, -0.0135, -0.0465, -0.0304,  0.0490, -0.0168,  0.0621,\n",
      "         0.0006,  0.0251, -0.0407, -0.0675,  0.0702,  0.0231,  0.0465, -0.0196,\n",
      "        -0.0330, -0.0325,  0.0059,  0.0548, -0.0502, -0.0152, -0.0483,  0.0588,\n",
      "         0.0521, -0.0110,  0.0271,  0.0293, -0.0519,  0.0025,  0.0226, -0.0312,\n",
      "        -0.0108, -0.0042,  0.0058,  0.0698, -0.0251, -0.0349, -0.0695,  0.0328,\n",
      "        -0.0135,  0.0320, -0.0706, -0.0381, -0.0394, -0.0251, -0.0421, -0.0431,\n",
      "        -0.0047, -0.0311, -0.0138, -0.0602, -0.0648, -0.0310,  0.0603,  0.0003,\n",
      "        -0.0229, -0.0321,  0.0586, -0.0534,  0.0194, -0.0541,  0.0369, -0.0080,\n",
      "        -0.0493, -0.0045, -0.0702, -0.0079,  0.0397, -0.0353, -0.0190,  0.0444,\n",
      "         0.0664,  0.0403, -0.0083,  0.0578, -0.0568, -0.0236, -0.0472,  0.0215,\n",
      "        -0.0496, -0.0503, -0.0126,  0.0678,  0.0664,  0.0528, -0.0123, -0.0644,\n",
      "        -0.0142,  0.0458, -0.0615, -0.0393,  0.0673, -0.0504,  0.0189,  0.0451],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0433, -0.0605, -0.0591,  ..., -0.0437,  0.0523,  0.0195],\n",
      "        [-0.0067, -0.0668,  0.0377,  ...,  0.0381,  0.0481, -0.0683],\n",
      "        [-0.0643, -0.0461,  0.0448,  ..., -0.0200, -0.0257, -0.0298],\n",
      "        ...,\n",
      "        [ 0.0323, -0.0306, -0.0503,  ...,  0.0248, -0.0067,  0.0420],\n",
      "        [-0.0542, -0.0467, -0.0588,  ...,  0.0584,  0.0390, -0.0090],\n",
      "        [ 0.0508,  0.0559, -0.0611,  ...,  0.0421, -0.0339,  0.0230]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0594, -0.0702, -0.0622,  ...,  0.0594, -0.0103, -0.0231],\n",
      "        [ 0.0686,  0.0344, -0.0535,  ...,  0.0577,  0.0550,  0.0699],\n",
      "        [-0.0191,  0.0687, -0.0130,  ...,  0.0281, -0.0269, -0.0113],\n",
      "        ...,\n",
      "        [-0.0192,  0.0181,  0.0311,  ..., -0.0336, -0.0492, -0.0050],\n",
      "        [ 0.0303, -0.0171,  0.0013,  ...,  0.0234, -0.0161, -0.0323],\n",
      "        [-0.0414, -0.0312,  0.0382,  ..., -0.0268, -0.0115,  0.0102]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 4.1490e-02, -5.3125e-02, -3.4340e-02,  5.9919e-02,  6.8520e-02,\n",
      "        -3.9341e-02, -6.3660e-02, -4.6045e-02,  3.7573e-02, -4.6872e-02,\n",
      "        -1.8574e-02, -6.1489e-03, -4.4822e-02,  1.4221e-02,  5.3103e-03,\n",
      "         2.4750e-02,  5.1898e-02, -6.5184e-02, -3.6166e-02,  5.9555e-02,\n",
      "        -6.6432e-02, -3.4098e-02, -4.0598e-02, -4.4732e-02, -3.6312e-02,\n",
      "         1.6727e-02, -3.8429e-02, -4.4192e-02,  2.9413e-02,  1.6992e-02,\n",
      "        -3.1597e-02,  3.0159e-02, -6.1660e-02,  2.6232e-02, -1.4708e-02,\n",
      "         4.8214e-02, -5.5629e-02, -8.4961e-03, -6.4345e-02, -3.4630e-02,\n",
      "        -5.5656e-02,  3.6831e-02,  2.6089e-02, -4.7436e-03, -6.9750e-03,\n",
      "        -2.6571e-02,  2.6368e-03, -4.8664e-03, -4.7801e-02,  3.3880e-02,\n",
      "        -5.5501e-02, -3.2285e-02, -4.5858e-02, -4.6959e-02, -1.5016e-02,\n",
      "        -6.1278e-02, -3.1888e-03, -6.1102e-02, -8.1988e-03,  7.3131e-03,\n",
      "         6.2135e-02, -4.1968e-02,  2.3505e-02, -4.6825e-02, -4.7112e-02,\n",
      "        -4.9846e-02,  5.2579e-02,  1.6148e-02, -1.0921e-02,  3.3608e-02,\n",
      "        -1.2910e-02, -5.0818e-02,  2.8880e-02,  1.9077e-02,  3.7880e-02,\n",
      "        -6.8446e-04, -2.8583e-02,  3.1921e-02, -2.5957e-02,  2.3200e-02,\n",
      "         4.1300e-02,  1.0310e-02,  5.9864e-02, -4.0014e-02,  4.3850e-02,\n",
      "         5.3316e-02, -5.4525e-02, -5.0735e-02,  9.9452e-03, -6.6493e-02,\n",
      "        -5.4518e-02, -3.3020e-02, -5.2995e-02, -2.7603e-02,  5.8286e-02,\n",
      "        -5.1128e-02, -8.9877e-03,  3.9989e-02, -6.2397e-02,  5.6230e-02,\n",
      "         2.2950e-02,  5.0839e-02,  3.0029e-02,  4.8023e-02, -4.0936e-02,\n",
      "         1.7203e-02,  1.8224e-03, -6.5536e-02,  2.9065e-02,  6.2822e-03,\n",
      "         6.8878e-02,  4.2153e-02, -5.3136e-02, -1.6894e-02, -5.0706e-02,\n",
      "         5.1019e-02,  5.2997e-02, -5.6325e-02,  7.8048e-03,  4.4014e-02,\n",
      "        -4.8361e-02, -1.7931e-02,  1.6279e-02,  6.2767e-02,  5.8929e-02,\n",
      "         4.6138e-04, -5.1285e-02, -4.9288e-02,  1.8123e-02, -3.5771e-02,\n",
      "         4.4768e-02,  6.1039e-02, -1.6888e-02, -5.7129e-02, -7.1659e-04,\n",
      "        -4.0246e-02, -6.0448e-02,  2.2731e-02, -2.8531e-02,  2.1475e-02,\n",
      "         2.8546e-02,  1.1341e-02,  2.1639e-02, -1.1828e-02,  6.9152e-03,\n",
      "         2.8942e-02,  5.9123e-02, -2.5574e-02, -7.8794e-03,  2.2162e-02,\n",
      "         2.7808e-02, -4.0296e-02, -3.5072e-02, -5.9715e-02,  5.6303e-02,\n",
      "         7.8263e-03,  8.9830e-03, -1.5412e-02,  6.0064e-02, -3.5314e-02,\n",
      "         2.8204e-02,  6.8603e-02,  2.3377e-02, -5.3178e-02, -7.0417e-02,\n",
      "         4.5540e-02,  3.4249e-02,  2.0692e-02, -3.7579e-02,  5.5305e-02,\n",
      "        -4.4564e-02,  5.3760e-02,  1.9223e-02, -4.9393e-02,  6.1178e-02,\n",
      "        -2.5098e-02,  6.2147e-02, -6.8859e-02,  6.5402e-02, -3.4955e-02,\n",
      "        -2.1173e-02,  5.1760e-02, -1.2179e-02, -2.7953e-02,  3.6638e-02,\n",
      "         2.0375e-02, -5.1272e-02,  5.7394e-02, -5.6597e-02,  2.2311e-02,\n",
      "        -4.9879e-02,  1.1262e-02, -5.0029e-02,  2.4845e-02,  5.9474e-02,\n",
      "        -1.9210e-03,  4.6664e-02,  1.5671e-02,  4.5006e-02,  1.1654e-02,\n",
      "         5.0905e-02, -2.4379e-02, -4.1112e-02, -3.2492e-02,  5.3565e-02,\n",
      "         2.4573e-02, -5.1709e-02,  1.2358e-02, -5.3221e-02, -4.9906e-03,\n",
      "        -1.9590e-02,  2.2537e-03,  4.3451e-02,  2.1619e-02,  5.5690e-02,\n",
      "        -2.5084e-02,  5.2927e-02, -3.0249e-02, -5.3840e-02, -4.2499e-02,\n",
      "        -5.4450e-02, -6.1528e-02,  4.5118e-02, -1.9221e-02,  5.2366e-02,\n",
      "        -1.7416e-02,  6.5525e-02,  2.0345e-02, -2.4425e-02, -3.9741e-02,\n",
      "        -3.4010e-02, -4.0094e-02,  3.0618e-02,  4.9194e-02,  3.0092e-02,\n",
      "        -1.7261e-02,  6.5067e-02, -6.8296e-02, -5.3934e-02,  5.0187e-03,\n",
      "        -5.2231e-02,  2.3119e-02,  4.6839e-02, -6.0219e-02,  3.9589e-02,\n",
      "        -5.9246e-02, -6.1967e-02,  2.9126e-02,  4.1131e-02,  4.7488e-02,\n",
      "        -4.4547e-03,  4.5171e-02, -2.9181e-03,  1.9404e-02,  5.2635e-02,\n",
      "         1.2777e-03,  2.9238e-02, -6.8635e-02,  2.9586e-03, -2.0607e-02,\n",
      "        -1.1641e-02, -1.9159e-02,  4.4154e-02, -2.7539e-02, -6.7357e-02,\n",
      "        -5.6629e-02,  2.4800e-02,  4.0626e-02, -5.0142e-02,  6.0480e-03,\n",
      "        -5.5308e-02, -2.8243e-02,  2.6679e-02,  3.4512e-02,  4.4548e-02,\n",
      "         7.9501e-03, -4.4010e-02,  5.6339e-02,  6.5917e-02, -4.7859e-02,\n",
      "        -7.0429e-02,  5.8018e-02, -3.7199e-02,  6.0132e-03, -4.0443e-02,\n",
      "         5.9508e-02, -2.4905e-02, -1.5727e-02, -2.6547e-02, -3.1431e-02,\n",
      "         5.5022e-02,  8.0082e-03, -6.5722e-02,  5.9817e-02,  2.9338e-02,\n",
      "        -4.9751e-02,  2.4221e-02, -6.8875e-02, -2.7721e-02,  3.7726e-03,\n",
      "        -1.5632e-02, -1.0246e-02, -3.7610e-02,  1.5039e-02, -6.3394e-02,\n",
      "        -6.5217e-02, -4.3340e-02, -3.8376e-02, -4.6878e-02,  3.7785e-02,\n",
      "        -5.9328e-02, -6.8627e-02,  6.4998e-02,  6.3273e-02, -2.5452e-02,\n",
      "         6.5660e-02,  5.3514e-02,  6.8859e-02, -2.9692e-02, -3.1659e-02,\n",
      "        -1.0715e-02,  4.8651e-02, -7.5658e-03,  5.7449e-02,  5.2004e-02,\n",
      "         1.3375e-02,  3.0880e-03,  5.2295e-02, -9.7333e-03,  4.1034e-02,\n",
      "         3.0965e-02, -5.2435e-02,  5.6121e-02,  2.1245e-02,  5.0180e-02,\n",
      "         3.6598e-02,  9.9194e-03, -3.2752e-02,  5.8620e-02,  5.3058e-02,\n",
      "        -6.5071e-03, -2.2497e-02,  2.8998e-02,  3.1549e-02,  2.4933e-02,\n",
      "         6.4492e-02,  5.3490e-02, -3.3066e-02,  3.9384e-04,  6.6203e-02,\n",
      "         1.8439e-02, -3.2057e-02, -1.2774e-02, -7.8913e-03,  1.7810e-02,\n",
      "        -3.3071e-02,  1.6310e-02,  2.4012e-03, -4.9233e-02, -4.8731e-02,\n",
      "        -1.0074e-03, -3.8177e-02, -1.2127e-02, -6.9477e-03, -3.3077e-02,\n",
      "         5.5102e-02,  1.1067e-02, -2.6184e-02, -5.4173e-02,  8.4554e-04,\n",
      "        -4.2856e-02, -2.7236e-02,  4.3307e-02, -3.8653e-02, -1.3050e-02,\n",
      "         6.2869e-02,  2.4117e-02, -2.5227e-03,  1.6789e-02, -2.1864e-02,\n",
      "         6.1601e-02, -3.2799e-02, -2.4317e-02,  2.2333e-03,  5.8843e-02,\n",
      "         3.4698e-02, -5.1322e-02,  6.2516e-02, -5.9575e-02,  1.5087e-02,\n",
      "        -4.8487e-02,  1.6876e-02, -8.9142e-03, -6.9930e-02,  5.8117e-02,\n",
      "         5.3055e-02,  5.3324e-02,  2.4378e-02,  2.3032e-02, -3.6550e-02,\n",
      "         6.4171e-02,  3.1698e-02,  6.6821e-02,  5.7729e-02,  4.7617e-04,\n",
      "        -3.8289e-02, -5.4928e-02, -1.4575e-02,  4.9649e-02, -1.8343e-03,\n",
      "         3.4092e-02,  4.9768e-02, -6.0883e-02, -6.6713e-02,  1.5590e-02,\n",
      "         7.0622e-02, -3.5285e-02,  1.2131e-02, -6.1121e-02,  5.9659e-02,\n",
      "         9.4423e-03, -5.0063e-03,  2.4697e-02,  1.4976e-06, -7.0345e-02,\n",
      "         5.1682e-02,  5.4343e-02, -2.1098e-02, -7.0314e-02,  6.6112e-02,\n",
      "        -1.8471e-02,  2.8787e-03,  2.3709e-02,  4.2061e-02, -5.3826e-02,\n",
      "        -5.6583e-02, -1.2238e-02,  5.3477e-02,  3.8097e-02,  1.5101e-02,\n",
      "         5.7953e-02,  5.3110e-03, -1.4158e-03,  6.1280e-03,  2.8302e-02,\n",
      "         1.8283e-02, -5.9713e-02, -5.0843e-02, -1.4000e-02, -2.3272e-02,\n",
      "        -6.4842e-02, -3.2994e-02,  6.2334e-02,  4.8042e-02,  4.6708e-03,\n",
      "        -4.5404e-02, -6.7399e-02,  3.9072e-02, -5.2837e-02, -4.7055e-02,\n",
      "         7.8368e-03,  1.4299e-02, -6.1710e-03, -4.4401e-02,  2.4053e-02,\n",
      "         6.4798e-02,  9.2613e-03,  3.7788e-02, -1.2541e-02,  2.3392e-02,\n",
      "        -2.5677e-02,  1.8819e-02,  4.3579e-02,  2.4701e-02,  3.2696e-02,\n",
      "         5.0579e-02,  8.0205e-03, -6.3848e-02, -7.7741e-03, -6.1001e-02,\n",
      "         6.2134e-02,  1.3077e-02, -4.0239e-02, -3.3483e-02,  3.4337e-02,\n",
      "         6.1221e-04, -1.6372e-02, -4.8649e-02,  6.8471e-02,  5.2028e-02,\n",
      "         5.2224e-02,  1.9406e-02, -4.2137e-02, -6.8311e-02, -5.1130e-02,\n",
      "        -5.7509e-02,  1.5823e-02, -1.2907e-02, -1.4247e-02, -3.2425e-02,\n",
      "         1.8532e-02, -4.8318e-02,  3.8822e-02, -5.6793e-02,  6.7581e-02,\n",
      "        -1.2710e-02,  2.6958e-02,  6.3892e-02, -3.8730e-02,  4.9043e-02,\n",
      "         2.1120e-02,  2.8582e-02, -6.6858e-02,  4.8852e-02, -1.5100e-02,\n",
      "        -4.0645e-02,  2.1967e-02, -7.0325e-02, -2.8847e-02, -1.4087e-02,\n",
      "        -2.5432e-02,  6.4080e-02, -2.3982e-02, -5.1853e-02,  5.1647e-02,\n",
      "         4.1930e-02,  1.8122e-02,  1.7959e-02,  4.5273e-03, -5.2282e-02,\n",
      "         1.1090e-02,  2.4157e-02, -4.7759e-02,  3.8131e-02,  5.1162e-02,\n",
      "        -3.0338e-02, -4.2556e-02,  6.9690e-03,  3.6089e-02, -5.5737e-02,\n",
      "         2.4105e-03, -3.2088e-02,  3.0375e-02, -2.3865e-02,  6.2046e-02,\n",
      "         2.2292e-02,  6.1050e-02,  2.7026e-02, -6.6275e-02, -4.6046e-02,\n",
      "         9.9913e-03, -4.8027e-02,  2.9695e-02, -6.4142e-02,  4.8076e-02,\n",
      "        -1.2328e-02, -1.9553e-02,  4.4806e-02, -6.8179e-02, -1.2264e-02,\n",
      "         1.4190e-02, -4.7421e-02,  2.3562e-02, -1.0380e-02, -4.4666e-02,\n",
      "         2.5926e-02,  7.0556e-02, -5.2322e-02,  4.2374e-02,  5.0711e-03,\n",
      "        -3.9552e-02,  3.8103e-02, -1.5090e-03, -6.5565e-02,  6.9177e-02,\n",
      "         3.3671e-02,  7.2447e-03, -1.4055e-02, -6.7315e-02,  4.9146e-02,\n",
      "         6.0246e-02, -1.3188e-02,  4.3059e-02,  1.2293e-02, -4.5623e-02,\n",
      "        -1.4862e-02,  6.6457e-02,  5.1501e-02, -6.6950e-02, -6.8575e-02,\n",
      "        -6.8132e-02,  1.7668e-02,  1.1443e-02,  6.2267e-02,  4.3781e-02,\n",
      "         9.2938e-03, -3.9496e-02, -6.1936e-02,  5.0405e-02, -7.8458e-03,\n",
      "         6.6611e-02, -3.3041e-02, -4.8002e-03, -1.8001e-02, -6.3395e-02,\n",
      "         5.5979e-02,  3.3537e-02, -4.3800e-02, -2.8453e-02, -1.1742e-02,\n",
      "         1.7566e-02, -4.6444e-03, -2.4744e-02, -6.4970e-02, -7.2053e-03,\n",
      "         6.3607e-02,  1.8452e-03, -3.6296e-02,  7.5639e-04, -3.6809e-02,\n",
      "         3.3085e-02,  4.3384e-02,  1.9128e-02, -6.9015e-02,  6.6958e-02,\n",
      "        -5.5743e-02, -1.9589e-02,  6.3544e-02,  2.8945e-02, -4.8619e-02,\n",
      "         6.3061e-02,  4.0733e-02, -1.1367e-02,  5.4091e-02, -6.5800e-02,\n",
      "         1.9365e-02,  6.8760e-02,  3.2994e-02,  5.2840e-03, -4.6131e-02,\n",
      "        -2.1961e-02, -8.6696e-03, -1.1921e-02,  5.7507e-02,  1.0744e-02,\n",
      "         1.3392e-02,  5.4763e-02, -6.2485e-03,  3.9450e-02,  5.8436e-02,\n",
      "        -5.2046e-02,  2.9630e-02, -1.9009e-03,  9.7910e-03, -3.2473e-02,\n",
      "        -8.3518e-03, -1.9836e-02, -3.7118e-02, -5.7028e-02,  6.7428e-02,\n",
      "         6.0489e-02, -6.9278e-02, -5.5880e-02,  6.0083e-02,  4.5164e-03,\n",
      "         2.6898e-02, -3.2104e-02, -5.7369e-02,  4.6658e-02, -6.3702e-02,\n",
      "        -3.4248e-02, -4.3552e-02, -4.1577e-02,  4.7652e-02,  2.2586e-02,\n",
      "         4.3335e-02, -6.7826e-02, -2.0993e-02,  3.1078e-02,  7.0170e-02,\n",
      "         4.3316e-02,  6.4771e-02,  6.8562e-02,  3.8280e-02,  1.3599e-02,\n",
      "         3.7019e-02,  3.0513e-02, -4.3327e-02,  3.8334e-02,  2.5581e-02,\n",
      "        -6.3498e-02,  4.7069e-02,  3.2689e-02, -4.1599e-02, -2.9369e-02,\n",
      "        -5.0356e-03,  5.6791e-02, -5.7932e-03, -1.2216e-02,  6.3357e-03,\n",
      "         4.7334e-02, -1.5475e-02, -1.3353e-02, -4.8618e-02,  3.1334e-02,\n",
      "         1.2250e-02,  1.3979e-02, -2.7915e-03,  2.1174e-02, -3.4431e-02,\n",
      "        -6.7645e-02,  1.4538e-03, -3.2475e-02, -1.5450e-02,  5.9809e-02,\n",
      "        -5.4147e-02, -3.0402e-03, -3.6375e-02,  2.7725e-02, -1.4577e-02,\n",
      "         9.8576e-03, -1.2893e-02,  2.7661e-02,  1.7235e-02, -1.0577e-02,\n",
      "         2.7431e-02,  5.6127e-02,  8.4240e-03, -5.5406e-02, -2.9814e-02,\n",
      "        -6.8910e-02,  6.5082e-02,  6.6192e-02,  6.3449e-02,  4.6936e-02,\n",
      "        -2.2684e-02, -4.6616e-02, -5.1865e-02,  3.9552e-02,  1.5168e-02,\n",
      "        -1.5756e-02, -6.9213e-02, -4.8154e-02, -5.3359e-02,  6.9575e-02,\n",
      "        -9.2525e-03,  8.8766e-03,  3.0654e-02,  7.0293e-02,  5.2458e-03,\n",
      "         5.9414e-02,  3.0369e-02, -1.2334e-02, -2.4059e-02, -3.6148e-02,\n",
      "        -2.3772e-02,  3.9673e-02,  6.5838e-02, -4.9601e-02, -2.5713e-02,\n",
      "         4.3523e-02, -3.9539e-02,  9.4459e-03, -5.6652e-02,  4.9661e-02,\n",
      "        -3.0070e-02, -5.3260e-03,  4.5782e-02,  9.9770e-03,  1.6714e-02,\n",
      "         6.0262e-02,  5.1724e-02, -2.8194e-02, -6.6112e-02, -2.4169e-02,\n",
      "        -5.6685e-02, -5.1571e-03, -2.8211e-02,  6.8333e-02,  4.8772e-02,\n",
      "        -6.6697e-02,  6.0154e-02,  1.9797e-02, -4.4839e-02,  3.9190e-02,\n",
      "        -6.4982e-02,  3.1074e-02, -7.4852e-03, -3.0553e-02,  1.1965e-02,\n",
      "        -6.0953e-02, -4.0618e-02, -5.5893e-02, -6.2174e-02,  1.1909e-02,\n",
      "         3.0237e-02, -5.0855e-02,  5.0527e-02,  6.9453e-02, -2.0166e-02],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0682, -0.0632,  0.0572,  0.0503, -0.0591, -0.0093,  0.0321,  0.0050,\n",
      "        -0.0612, -0.0635,  0.0438,  0.0039,  0.0373,  0.0145,  0.0085, -0.0540,\n",
      "        -0.0390,  0.0193,  0.0354,  0.0432,  0.0028, -0.0364,  0.0237,  0.0520,\n",
      "         0.0663,  0.0602, -0.0223,  0.0509,  0.0076, -0.0140,  0.0184,  0.0630,\n",
      "         0.0322,  0.0560,  0.0648,  0.0454, -0.0291,  0.0084,  0.0590,  0.0335,\n",
      "        -0.0001, -0.0161,  0.0042, -0.0544,  0.0598,  0.0328,  0.0373,  0.0068,\n",
      "        -0.0214,  0.0202,  0.0047, -0.0439,  0.0070, -0.0364, -0.0380,  0.0289,\n",
      "        -0.0311, -0.0667,  0.0293, -0.0394,  0.0039, -0.0158,  0.0427,  0.0276,\n",
      "        -0.0019, -0.0669, -0.0356,  0.0461, -0.0087, -0.0181, -0.0566,  0.0099,\n",
      "        -0.0265, -0.0666,  0.0514, -0.0617,  0.0539,  0.0564,  0.0078, -0.0496,\n",
      "         0.0328, -0.0618, -0.0145,  0.0511,  0.0368, -0.0429,  0.0676, -0.0636,\n",
      "        -0.0246, -0.0515, -0.0644,  0.0186, -0.0627,  0.0338, -0.0587,  0.0657,\n",
      "         0.0124, -0.0040, -0.0673, -0.0090, -0.0553, -0.0363,  0.0150, -0.0661,\n",
      "         0.0240,  0.0556,  0.0506,  0.0299, -0.0084, -0.0030, -0.0354, -0.0347,\n",
      "         0.0150, -0.0560, -0.0572,  0.0411,  0.0563,  0.0584,  0.0570,  0.0019,\n",
      "         0.0042,  0.0097,  0.0602,  0.0075,  0.0133,  0.0081, -0.0348, -0.0648,\n",
      "        -0.0352,  0.0301,  0.0246,  0.0042,  0.0041, -0.0085,  0.0234, -0.0434,\n",
      "        -0.0296,  0.0301,  0.0476, -0.0583,  0.0034, -0.0288,  0.0703, -0.0001,\n",
      "        -0.0569, -0.0400, -0.0050,  0.0119, -0.0042, -0.0325,  0.0029, -0.0676,\n",
      "         0.0392, -0.0624,  0.0293, -0.0498, -0.0444, -0.0382, -0.0613,  0.0185,\n",
      "         0.0701,  0.0452,  0.0027, -0.0650,  0.0439,  0.0635,  0.0151, -0.0685,\n",
      "         0.0152, -0.0169,  0.0525,  0.0617, -0.0134, -0.0586,  0.0200, -0.0257,\n",
      "         0.0226, -0.0030, -0.0518, -0.0545,  0.0683,  0.0490, -0.0562,  0.0346,\n",
      "        -0.0090, -0.0067,  0.0569,  0.0422,  0.0158, -0.0012, -0.0625, -0.0297,\n",
      "        -0.0507, -0.0662,  0.0062,  0.0621,  0.0363,  0.0432,  0.0375,  0.0465,\n",
      "        -0.0063,  0.0140, -0.0436,  0.0225, -0.0512,  0.0675, -0.0701, -0.0297,\n",
      "         0.0120,  0.0136, -0.0100, -0.0478,  0.0161,  0.0166,  0.0691, -0.0005,\n",
      "         0.0397, -0.0491,  0.0270,  0.0472,  0.0542, -0.0516, -0.0660,  0.0163,\n",
      "        -0.0124, -0.0030,  0.0211,  0.0033, -0.0330,  0.0236, -0.0639,  0.0460,\n",
      "         0.0568, -0.0244,  0.0061, -0.0044,  0.0253, -0.0372,  0.0492,  0.0540,\n",
      "         0.0444, -0.0460, -0.0503,  0.0375, -0.0311, -0.0541, -0.0221, -0.0013,\n",
      "         0.0588, -0.0354,  0.0457, -0.0043,  0.0613, -0.0305,  0.0352, -0.0612,\n",
      "         0.0668,  0.0049,  0.0303,  0.0603, -0.0400, -0.0525, -0.0203,  0.0547,\n",
      "         0.0268,  0.0586,  0.0126, -0.0263,  0.0318, -0.0253,  0.0272, -0.0504,\n",
      "        -0.0049,  0.0050, -0.0265, -0.0049,  0.0698, -0.0612, -0.0334,  0.0007,\n",
      "         0.0023, -0.0464,  0.0563, -0.0097,  0.0619, -0.0388,  0.0230,  0.0471,\n",
      "        -0.0492,  0.0103, -0.0445, -0.0287, -0.0303, -0.0645, -0.0559,  0.0151,\n",
      "        -0.0701, -0.0077, -0.0019,  0.0450,  0.0045,  0.0647,  0.0166, -0.0569,\n",
      "        -0.0342,  0.0623, -0.0255, -0.0125, -0.0208, -0.0034,  0.0101,  0.0576,\n",
      "         0.0023, -0.0104, -0.0277, -0.0261,  0.0087,  0.0123, -0.0161,  0.0236,\n",
      "         0.0003, -0.0496, -0.0142, -0.0684, -0.0051, -0.0096,  0.0493, -0.0510,\n",
      "        -0.0199, -0.0638,  0.0585,  0.0418,  0.0388,  0.0149,  0.0350, -0.0317,\n",
      "        -0.0275,  0.0093, -0.0354, -0.0131, -0.0622, -0.0395,  0.0605,  0.0505,\n",
      "         0.0424,  0.0235,  0.0413, -0.0049,  0.0034,  0.0525, -0.0127,  0.0333,\n",
      "         0.0226, -0.0697,  0.0236,  0.0102, -0.0676,  0.0309,  0.0391, -0.0296,\n",
      "        -0.0676, -0.0281,  0.0412,  0.0247, -0.0339, -0.0561,  0.0297,  0.0598,\n",
      "         0.0698, -0.0553,  0.0533, -0.0138, -0.0114,  0.0124,  0.0093, -0.0327,\n",
      "        -0.0130, -0.0666, -0.0307,  0.0168, -0.0230,  0.0512,  0.0700,  0.0253,\n",
      "         0.0341, -0.0456, -0.0074,  0.0008, -0.0434, -0.0099, -0.0286,  0.0463,\n",
      "         0.0496,  0.0577,  0.0470,  0.0451, -0.0381, -0.0352,  0.0018, -0.0315,\n",
      "         0.0181,  0.0091,  0.0231, -0.0177,  0.0633,  0.0380,  0.0457, -0.0087,\n",
      "         0.0668,  0.0662,  0.0581, -0.0605,  0.0211, -0.0262,  0.0280, -0.0165,\n",
      "         0.0282,  0.0382, -0.0331,  0.0085, -0.0350,  0.0194, -0.0283, -0.0091,\n",
      "         0.0565,  0.0049, -0.0378,  0.0632,  0.0328,  0.0669,  0.0007, -0.0230,\n",
      "        -0.0593, -0.0697, -0.0066, -0.0316, -0.0278, -0.0194, -0.0509, -0.0137,\n",
      "        -0.0043,  0.0094, -0.0557,  0.0155, -0.0447,  0.0195,  0.0377,  0.0404,\n",
      "         0.0523, -0.0114,  0.0427, -0.0402,  0.0409,  0.0479, -0.0329,  0.0028,\n",
      "        -0.0678, -0.0695,  0.0064,  0.0283, -0.0268,  0.0046, -0.0013,  0.0047,\n",
      "         0.0365, -0.0007, -0.0595,  0.0091, -0.0205, -0.0258,  0.0187, -0.0540,\n",
      "        -0.0138,  0.0296,  0.0372,  0.0627,  0.0688,  0.0577,  0.0361,  0.0208,\n",
      "        -0.0556, -0.0530,  0.0509, -0.0451, -0.0520,  0.0589,  0.0119, -0.0290,\n",
      "         0.0388, -0.0053, -0.0339,  0.0346,  0.0400, -0.0648, -0.0275, -0.0084,\n",
      "        -0.0349,  0.0506, -0.0125,  0.0315, -0.0697, -0.0368, -0.0372,  0.0077,\n",
      "         0.0013,  0.0134, -0.0407, -0.0385,  0.0356, -0.0017,  0.0685,  0.0111,\n",
      "         0.0309,  0.0224, -0.0210,  0.0453,  0.0039,  0.0225,  0.0526,  0.0097,\n",
      "         0.0303, -0.0279, -0.0017,  0.0678, -0.0261,  0.0056,  0.0314,  0.0621,\n",
      "        -0.0561,  0.0269, -0.0066, -0.0180, -0.0515,  0.0420,  0.0023,  0.0411,\n",
      "        -0.0375,  0.0105, -0.0673, -0.0672,  0.0106,  0.0651,  0.0468,  0.0519,\n",
      "        -0.0601, -0.0474, -0.0085,  0.0332, -0.0461,  0.0171,  0.0123, -0.0389,\n",
      "         0.0613, -0.0414,  0.0170, -0.0508, -0.0628, -0.0286,  0.0671,  0.0290,\n",
      "        -0.0661, -0.0694, -0.0683, -0.0453, -0.0186, -0.0418, -0.0604,  0.0192,\n",
      "         0.0129, -0.0415, -0.0380,  0.0103,  0.0181, -0.0470,  0.0105, -0.0312,\n",
      "        -0.0436,  0.0219,  0.0455,  0.0231,  0.0627, -0.0055,  0.0394, -0.0256,\n",
      "        -0.0644,  0.0017, -0.0396,  0.0024, -0.0593, -0.0387,  0.0162, -0.0084,\n",
      "         0.0035, -0.0428, -0.0101, -0.0073,  0.0249, -0.0128, -0.0301,  0.0307,\n",
      "         0.0211, -0.0068, -0.0343,  0.0110, -0.0435, -0.0107,  0.0701, -0.0554,\n",
      "        -0.0525,  0.0664,  0.0603, -0.0612,  0.0268, -0.0529,  0.0132, -0.0510,\n",
      "        -0.0639, -0.0134, -0.0577,  0.0352,  0.0437, -0.0611, -0.0419,  0.0044,\n",
      "        -0.0481,  0.0369, -0.0478,  0.0580, -0.0477, -0.0217, -0.0212, -0.0595,\n",
      "         0.0069,  0.0671, -0.0335, -0.0113,  0.0414,  0.0285, -0.0145,  0.0336,\n",
      "        -0.0507, -0.0286,  0.0570, -0.0261,  0.0546,  0.0044, -0.0631, -0.0146,\n",
      "         0.0533, -0.0415,  0.0118,  0.0339,  0.0276, -0.0110,  0.0469, -0.0424,\n",
      "        -0.0425,  0.0547, -0.0546, -0.0053,  0.0670, -0.0421,  0.0654, -0.0126,\n",
      "        -0.0538,  0.0377, -0.0301,  0.0587,  0.0167,  0.0290,  0.0018, -0.0232,\n",
      "        -0.0059,  0.0686, -0.0513, -0.0040, -0.0576,  0.0515,  0.0578,  0.0100,\n",
      "         0.0366, -0.0295,  0.0152,  0.0512, -0.0336,  0.0319,  0.0265,  0.0270,\n",
      "         0.0275, -0.0475, -0.0419, -0.0583,  0.0323, -0.0502, -0.0100, -0.0706,\n",
      "         0.0189,  0.0536, -0.0453,  0.0273, -0.0392, -0.0570, -0.0385, -0.0251,\n",
      "         0.0481,  0.0357, -0.0612, -0.0250,  0.0450, -0.0141,  0.0587,  0.0473,\n",
      "         0.0169, -0.0182,  0.0524, -0.0250,  0.0396, -0.0144,  0.0386, -0.0367,\n",
      "        -0.0189,  0.0702,  0.0630, -0.0020,  0.0328, -0.0451, -0.0369, -0.0333,\n",
      "         0.0137,  0.0611, -0.0461, -0.0271, -0.0519,  0.0001, -0.0352,  0.0155,\n",
      "         0.0007, -0.0292,  0.0371, -0.0644,  0.0583, -0.0620, -0.0489,  0.0348,\n",
      "         0.0474, -0.0339,  0.0199, -0.0361,  0.0192, -0.0163,  0.0686, -0.0484,\n",
      "        -0.0147, -0.0661,  0.0660,  0.0329,  0.0490,  0.0094, -0.0376,  0.0122,\n",
      "        -0.0014, -0.0157,  0.0504,  0.0224, -0.0698, -0.0425,  0.0291,  0.0108,\n",
      "        -0.0340,  0.0220, -0.0289, -0.0328, -0.0571, -0.0175, -0.0482, -0.0508,\n",
      "        -0.0116,  0.0666,  0.0298, -0.0641, -0.0604,  0.0667,  0.0007,  0.0266,\n",
      "        -0.0251,  0.0289, -0.0125, -0.0679, -0.0033,  0.0145, -0.0033,  0.0013,\n",
      "        -0.0490,  0.0115,  0.0264,  0.0371,  0.0379, -0.0090, -0.0556,  0.0648],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[ 0.0655,  0.0454,  0.0080,  ..., -0.0265,  0.0283, -0.0215],\n",
      "        [ 0.0063, -0.0193, -0.0195,  ..., -0.0585,  0.0376,  0.0041],\n",
      "        [ 0.0428, -0.0701, -0.0237,  ...,  0.0185,  0.0308, -0.0272],\n",
      "        ...,\n",
      "        [-0.0178,  0.0446,  0.0129,  ..., -0.0523,  0.0247,  0.0623],\n",
      "        [-0.0255,  0.0269,  0.0139,  ...,  0.0645, -0.0335,  0.0333],\n",
      "        [-0.0072,  0.0300,  0.0597,  ..., -0.0107, -0.0286,  0.0454]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0472, -0.0627, -0.0622,  ...,  0.0289,  0.0500,  0.0161],\n",
      "        [ 0.0628, -0.0527, -0.0353,  ...,  0.0175, -0.0051, -0.0548],\n",
      "        [-0.0165, -0.0520,  0.0193,  ...,  0.0139, -0.0421,  0.0222],\n",
      "        ...,\n",
      "        [ 0.0057, -0.0458, -0.0113,  ...,  0.0569, -0.0471,  0.0221],\n",
      "        [ 0.0106, -0.0351,  0.0399,  ..., -0.0144,  0.0450, -0.0186],\n",
      "        [ 0.0236, -0.0030,  0.0650,  ..., -0.0353,  0.0277, -0.0100]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0535, -0.0289, -0.0347,  0.0008, -0.0664,  0.0269, -0.0382,  0.0240,\n",
      "         0.0558,  0.0127, -0.0666, -0.0059,  0.0169, -0.0151,  0.0039,  0.0471,\n",
      "         0.0130, -0.0048, -0.0243,  0.0533, -0.0459, -0.0551,  0.0384, -0.0461,\n",
      "        -0.0071,  0.0076,  0.0479, -0.0304, -0.0217, -0.0186, -0.0200,  0.0627,\n",
      "         0.0219,  0.0026, -0.0202,  0.0488,  0.0278,  0.0313, -0.0585, -0.0074,\n",
      "         0.0609,  0.0610,  0.0623, -0.0295, -0.0010,  0.0471,  0.0069,  0.0312,\n",
      "         0.0650, -0.0049,  0.0131, -0.0261,  0.0624, -0.0116,  0.0267,  0.0377,\n",
      "         0.0488,  0.0255,  0.0307, -0.0225, -0.0660,  0.0356, -0.0006, -0.0693,\n",
      "        -0.0295, -0.0626, -0.0022, -0.0702,  0.0397, -0.0680,  0.0250,  0.0180,\n",
      "        -0.0125, -0.0074,  0.0221, -0.0217, -0.0555,  0.0665, -0.0203, -0.0704,\n",
      "         0.0547,  0.0451,  0.0487, -0.0446, -0.0441, -0.0448, -0.0686,  0.0427,\n",
      "        -0.0522,  0.0216,  0.0056, -0.0471,  0.0649, -0.0611, -0.0704,  0.0230,\n",
      "        -0.0659, -0.0646, -0.0126,  0.0658,  0.0385,  0.0439, -0.0369, -0.0383,\n",
      "        -0.0311,  0.0558, -0.0687,  0.0436,  0.0457, -0.0705, -0.0117,  0.0392,\n",
      "         0.0152, -0.0362, -0.0545,  0.0693,  0.0311, -0.0075, -0.0188,  0.0211,\n",
      "         0.0252,  0.0310,  0.0653,  0.0612,  0.0109, -0.0240, -0.0673, -0.0365,\n",
      "        -0.0253, -0.0615, -0.0192, -0.0292,  0.0689, -0.0659, -0.0451, -0.0209,\n",
      "         0.0569,  0.0524,  0.0191,  0.0239, -0.0001, -0.0153,  0.0044, -0.0648,\n",
      "        -0.0610, -0.0705,  0.0255,  0.0294, -0.0592,  0.0320,  0.0168,  0.0145,\n",
      "         0.0055, -0.0506,  0.0062,  0.0017, -0.0403,  0.0198,  0.0056, -0.0596,\n",
      "         0.0671,  0.0253,  0.0338, -0.0452, -0.0205, -0.0509,  0.0491, -0.0493,\n",
      "        -0.0130,  0.0555, -0.0346, -0.0262, -0.0433, -0.0504, -0.0649, -0.0424,\n",
      "        -0.0103, -0.0052,  0.0417,  0.0024,  0.0347, -0.0014,  0.0246, -0.0466,\n",
      "        -0.0648,  0.0030,  0.0350,  0.0701,  0.0006,  0.0286, -0.0329,  0.0427,\n",
      "        -0.0023, -0.0311,  0.0693,  0.0303, -0.0005,  0.0527, -0.0087, -0.0545,\n",
      "        -0.0491,  0.0323, -0.0579, -0.0183, -0.0686,  0.0077,  0.0468,  0.0210,\n",
      "        -0.0153, -0.0121, -0.0069,  0.0264, -0.0175,  0.0509,  0.0667,  0.0372,\n",
      "        -0.0234,  0.0574,  0.0257,  0.0259,  0.0249, -0.0690,  0.0635, -0.0263,\n",
      "         0.0637,  0.0061,  0.0360,  0.0351, -0.0067,  0.0049, -0.0591, -0.0449,\n",
      "         0.0251, -0.0024, -0.0605, -0.0495,  0.0653,  0.0081,  0.0489, -0.0036,\n",
      "         0.0364,  0.0171, -0.0472, -0.0610, -0.0084,  0.0493,  0.0094, -0.0076,\n",
      "         0.0521,  0.0576,  0.0677, -0.0203,  0.0396,  0.0704,  0.0501,  0.0687,\n",
      "        -0.0242,  0.0644, -0.0564, -0.0622, -0.0682,  0.0233,  0.0441,  0.0084,\n",
      "         0.0656, -0.0609, -0.0288, -0.0032,  0.0172, -0.0627,  0.0325,  0.0469,\n",
      "        -0.0070,  0.0510,  0.0627, -0.0578,  0.0311, -0.0375, -0.0281, -0.0292,\n",
      "        -0.0628,  0.0206,  0.0250, -0.0633, -0.0079, -0.0229,  0.0624,  0.0154,\n",
      "        -0.0622, -0.0060,  0.0599,  0.0322, -0.0332, -0.0036, -0.0125,  0.0650,\n",
      "         0.0445, -0.0396, -0.0516,  0.0701,  0.0409, -0.0037,  0.0307, -0.0683,\n",
      "         0.0168,  0.0573,  0.0522,  0.0552,  0.0700, -0.0333,  0.0390, -0.0483,\n",
      "        -0.0468,  0.0294,  0.0063, -0.0257,  0.0323, -0.0262, -0.0440,  0.0480,\n",
      "        -0.0293, -0.0525,  0.0171,  0.0192,  0.0161, -0.0037,  0.0217,  0.0082,\n",
      "        -0.0664,  0.0007,  0.0528,  0.0047, -0.0030,  0.0024, -0.0172,  0.0124,\n",
      "        -0.0091, -0.0201, -0.0652, -0.0231,  0.0354,  0.0594, -0.0433, -0.0096,\n",
      "         0.0219,  0.0546,  0.0304, -0.0250, -0.0151,  0.0100, -0.0219,  0.0035,\n",
      "        -0.0064,  0.0313,  0.0259,  0.0035, -0.0547,  0.0663, -0.0132,  0.0316,\n",
      "        -0.0236, -0.0669,  0.0313, -0.0450, -0.0330,  0.0374,  0.0668,  0.0572,\n",
      "         0.0404, -0.0300,  0.0513, -0.0389,  0.0184, -0.0639, -0.0531,  0.0098,\n",
      "         0.0418, -0.0337, -0.0388,  0.0238,  0.0168,  0.0706, -0.0653, -0.0491,\n",
      "        -0.0157,  0.0295, -0.0242,  0.0679,  0.0496,  0.0274, -0.0471,  0.0253,\n",
      "         0.0309, -0.0522, -0.0182, -0.0589, -0.0582,  0.0215, -0.0524, -0.0114,\n",
      "        -0.0584, -0.0577, -0.0504, -0.0391,  0.0691,  0.0054, -0.0172,  0.0017,\n",
      "        -0.0312,  0.0170,  0.0108, -0.0675,  0.0682,  0.0174,  0.0194,  0.0420,\n",
      "         0.0691, -0.0633,  0.0296, -0.0670, -0.0504,  0.0459,  0.0199,  0.0256,\n",
      "        -0.0063,  0.0032,  0.0607, -0.0193, -0.0002, -0.0540,  0.0453,  0.0493,\n",
      "         0.0359,  0.0080, -0.0251,  0.0522,  0.0147, -0.0461,  0.0425,  0.0602,\n",
      "         0.0003, -0.0302,  0.0688,  0.0147, -0.0166,  0.0412,  0.0457,  0.0233,\n",
      "        -0.0398,  0.0188,  0.0041, -0.0447, -0.0402,  0.0483, -0.0696, -0.0668,\n",
      "        -0.0304, -0.0592, -0.0338,  0.0178,  0.0030,  0.0298,  0.0271,  0.0213,\n",
      "        -0.0399,  0.0140,  0.0321,  0.0114, -0.0687, -0.0679, -0.0331,  0.0613,\n",
      "         0.0387, -0.0273,  0.0154, -0.0290, -0.0147,  0.0594,  0.0098, -0.0253,\n",
      "         0.0249,  0.0073,  0.0223,  0.0545,  0.0401,  0.0571,  0.0561, -0.0227,\n",
      "        -0.0495,  0.0244, -0.0076,  0.0069,  0.0707,  0.0451,  0.0238,  0.0443,\n",
      "        -0.0253,  0.0259, -0.0179, -0.0130, -0.0667,  0.0026, -0.0129,  0.0089,\n",
      "         0.0606,  0.0262,  0.0369,  0.0500, -0.0260,  0.0082,  0.0177, -0.0579,\n",
      "         0.0322, -0.0209,  0.0090,  0.0491,  0.0502, -0.0192,  0.0166,  0.0219,\n",
      "         0.0045,  0.0497, -0.0058, -0.0543, -0.0675, -0.0126,  0.0207,  0.0229,\n",
      "         0.0009,  0.0268, -0.0339,  0.0275, -0.0650, -0.0079,  0.0617, -0.0174,\n",
      "         0.0606, -0.0040, -0.0582,  0.0316,  0.0061, -0.0078, -0.0647,  0.0418,\n",
      "        -0.0209,  0.0078,  0.0214,  0.0176, -0.0518,  0.0633,  0.0072,  0.0263,\n",
      "        -0.0125,  0.0407,  0.0659,  0.0038,  0.0232, -0.0091, -0.0324, -0.0433,\n",
      "        -0.0070,  0.0518,  0.0173, -0.0653, -0.0485,  0.0405, -0.0073,  0.0269,\n",
      "        -0.0540,  0.0155,  0.0419,  0.0237, -0.0436,  0.0592,  0.0038,  0.0700,\n",
      "         0.0483,  0.0691, -0.0578, -0.0659, -0.0555, -0.0167,  0.0557,  0.0002,\n",
      "        -0.0171,  0.0114,  0.0415,  0.0529, -0.0138, -0.0031,  0.0246,  0.0196,\n",
      "         0.0660, -0.0154, -0.0503,  0.0694,  0.0375,  0.0191,  0.0175, -0.0615,\n",
      "         0.0220,  0.0237,  0.0241,  0.0654,  0.0353,  0.0374, -0.0262,  0.0247,\n",
      "         0.0456, -0.0151,  0.0650, -0.0531, -0.0071,  0.0399, -0.0229, -0.0309,\n",
      "         0.0533,  0.0310,  0.0071,  0.0097, -0.0068,  0.0509,  0.0560, -0.0469,\n",
      "         0.0613,  0.0003, -0.0015, -0.0377,  0.0080, -0.0235,  0.0703,  0.0331,\n",
      "         0.0169,  0.0152,  0.0422, -0.0696, -0.0272, -0.0160,  0.0307, -0.0222,\n",
      "        -0.0057,  0.0154,  0.0567,  0.0653,  0.0215, -0.0057, -0.0492, -0.0333,\n",
      "        -0.0429, -0.0683, -0.0650, -0.0325, -0.0683, -0.0182, -0.0070,  0.0016,\n",
      "        -0.0654, -0.0330,  0.0372, -0.0319, -0.0607, -0.0307,  0.0522,  0.0486,\n",
      "        -0.0029, -0.0682, -0.0324, -0.0143, -0.0500, -0.0626, -0.0506,  0.0177,\n",
      "        -0.0493, -0.0434,  0.0606, -0.0470, -0.0212, -0.0318,  0.0510, -0.0516,\n",
      "        -0.0684,  0.0046, -0.0384,  0.0256,  0.0395, -0.0180,  0.0660,  0.0331,\n",
      "        -0.0569, -0.0333, -0.0629,  0.0195,  0.0154,  0.0625,  0.0402, -0.0502,\n",
      "         0.0546, -0.0231,  0.0539,  0.0668, -0.0223,  0.0429,  0.0082, -0.0196,\n",
      "        -0.0578,  0.0139, -0.0084,  0.0289,  0.0161,  0.0473, -0.0681,  0.0058,\n",
      "        -0.0138,  0.0394, -0.0009,  0.0230,  0.0375,  0.0296,  0.0123,  0.0488,\n",
      "         0.0224, -0.0557,  0.0600,  0.0393, -0.0439,  0.0067, -0.0133,  0.0537,\n",
      "        -0.0192,  0.0064,  0.0067,  0.0690, -0.0577, -0.0329, -0.0361, -0.0514,\n",
      "        -0.0320, -0.0528,  0.0134, -0.0129, -0.0097,  0.0379, -0.0103, -0.0357,\n",
      "         0.0174,  0.0091,  0.0532,  0.0600, -0.0448,  0.0365,  0.0251, -0.0564,\n",
      "         0.0215, -0.0596,  0.0083, -0.0150, -0.0255,  0.0606, -0.0495, -0.0298,\n",
      "         0.0040,  0.0657, -0.0166,  0.0420, -0.0333, -0.0684,  0.0040, -0.0486,\n",
      "         0.0534, -0.0041,  0.0540, -0.0232,  0.0418, -0.0649, -0.0578,  0.0433,\n",
      "        -0.0055, -0.0097,  0.0687,  0.0641, -0.0509, -0.0705,  0.0492,  0.0133,\n",
      "         0.0396, -0.0318, -0.0571, -0.0262,  0.0246,  0.0526,  0.0189,  0.0558,\n",
      "        -0.0407,  0.0414, -0.0513,  0.0628, -0.0702, -0.0153, -0.0366, -0.0587],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0359, -0.0531, -0.0668,  0.0665,  0.0081, -0.0679, -0.0490,  0.0239,\n",
      "        -0.0348, -0.0029, -0.0555,  0.0388,  0.0416, -0.0541, -0.0158,  0.0117,\n",
      "         0.0402,  0.0690,  0.0311, -0.0703, -0.0289, -0.0534, -0.0284,  0.0365,\n",
      "         0.0551, -0.0136, -0.0024, -0.0557,  0.0280, -0.0141,  0.0531,  0.0229,\n",
      "        -0.0177, -0.0498,  0.0106,  0.0360,  0.0353, -0.0411, -0.0406,  0.0562,\n",
      "        -0.0200,  0.0324,  0.0500, -0.0479, -0.0678,  0.0066, -0.0587, -0.0531,\n",
      "         0.0163,  0.0337,  0.0370, -0.0033,  0.0678, -0.0526, -0.0323, -0.0123,\n",
      "         0.0250,  0.0464, -0.0488, -0.0269,  0.0587,  0.0696,  0.0232, -0.0651,\n",
      "        -0.0645, -0.0252, -0.0535,  0.0253, -0.0263,  0.0606,  0.0302,  0.0677,\n",
      "        -0.0520,  0.0293, -0.0081,  0.0661,  0.0551, -0.0509, -0.0624, -0.0415,\n",
      "        -0.0406, -0.0656, -0.0284, -0.0631,  0.0267,  0.0571, -0.0690, -0.0069,\n",
      "        -0.0435,  0.0607,  0.0413,  0.0556,  0.0526, -0.0465,  0.0649, -0.0592,\n",
      "         0.0362, -0.0083,  0.0551, -0.0019, -0.0181, -0.0480, -0.0110, -0.0602,\n",
      "         0.0161,  0.0097, -0.0441, -0.0097,  0.0223, -0.0272,  0.0705, -0.0001,\n",
      "         0.0082,  0.0314,  0.0669,  0.0241, -0.0694,  0.0174, -0.0590,  0.0291,\n",
      "         0.0188,  0.0092,  0.0092, -0.0378,  0.0667,  0.0516,  0.0042,  0.0170,\n",
      "         0.0530, -0.0181,  0.0687,  0.0032,  0.0295,  0.0258,  0.0351, -0.0280,\n",
      "        -0.0492,  0.0379, -0.0619,  0.0582, -0.0206, -0.0310, -0.0639, -0.0266,\n",
      "        -0.0638, -0.0189,  0.0339, -0.0469, -0.0154, -0.0007, -0.0630,  0.0483,\n",
      "        -0.0441, -0.0427, -0.0245, -0.0322, -0.0119, -0.0521,  0.0557, -0.0568,\n",
      "        -0.0015,  0.0423, -0.0278,  0.0375,  0.0388, -0.0536,  0.0594,  0.0168,\n",
      "        -0.0358, -0.0124,  0.0141, -0.0020,  0.0625, -0.0556,  0.0386,  0.0517,\n",
      "         0.0459,  0.0345,  0.0475, -0.0011, -0.0120,  0.0561,  0.0012, -0.0340,\n",
      "        -0.0100,  0.0296,  0.0585, -0.0103, -0.0453, -0.0055,  0.0221,  0.0455,\n",
      "         0.0064,  0.0531,  0.0143,  0.0122, -0.0500, -0.0640,  0.0473,  0.0560,\n",
      "         0.0398, -0.0188, -0.0553, -0.0147, -0.0540,  0.0620,  0.0676, -0.0229,\n",
      "         0.0087,  0.0423,  0.0198,  0.0370,  0.0108, -0.0221, -0.0329, -0.0341,\n",
      "         0.0502,  0.0499,  0.0179, -0.0505,  0.0665,  0.0338, -0.0254,  0.0107,\n",
      "        -0.0442, -0.0028,  0.0676, -0.0033, -0.0080,  0.0651, -0.0057,  0.0303,\n",
      "         0.0164,  0.0297, -0.0050, -0.0422, -0.0401,  0.0597, -0.0014, -0.0382,\n",
      "        -0.0196,  0.0173,  0.0506,  0.0421,  0.0447, -0.0638, -0.0700,  0.0602,\n",
      "        -0.0690, -0.0393, -0.0003,  0.0184, -0.0168, -0.0439, -0.0262, -0.0137,\n",
      "         0.0187, -0.0058,  0.0048,  0.0532,  0.0005, -0.0304,  0.0006,  0.0661,\n",
      "        -0.0505, -0.0350,  0.0563, -0.0206, -0.0248,  0.0592, -0.0680, -0.0385,\n",
      "         0.0534, -0.0603,  0.0168, -0.0559,  0.0521,  0.0001,  0.0559, -0.0453,\n",
      "        -0.0522, -0.0258,  0.0688,  0.0286, -0.0193,  0.0445, -0.0491,  0.0410,\n",
      "        -0.0161,  0.0265,  0.0088,  0.0382,  0.0403, -0.0281, -0.0613, -0.0264,\n",
      "        -0.0698,  0.0614,  0.0393,  0.0461, -0.0459, -0.0028,  0.0545, -0.0632,\n",
      "         0.0107,  0.0590,  0.0680,  0.0298,  0.0384,  0.0057, -0.0415, -0.0539,\n",
      "         0.0338, -0.0050, -0.0412, -0.0647,  0.0425,  0.0369, -0.0435, -0.0442,\n",
      "         0.0693, -0.0037, -0.0586,  0.0172,  0.0596,  0.0164, -0.0529,  0.0001,\n",
      "        -0.0208,  0.0336,  0.0637, -0.0527, -0.0052,  0.0399, -0.0120, -0.0168,\n",
      "        -0.0163,  0.0072,  0.0014, -0.0352, -0.0294, -0.0032,  0.0208,  0.0018,\n",
      "         0.0321, -0.0672, -0.0231,  0.0001, -0.0658,  0.0327,  0.0502,  0.0171,\n",
      "         0.0322,  0.0613, -0.0305, -0.0348, -0.0467, -0.0231, -0.0460,  0.0188,\n",
      "        -0.0582,  0.0235,  0.0223, -0.0582, -0.0113, -0.0137,  0.0264, -0.0639,\n",
      "         0.0171, -0.0358,  0.0011,  0.0041,  0.0675, -0.0697,  0.0618,  0.0291,\n",
      "        -0.0704, -0.0334, -0.0605,  0.0002,  0.0511, -0.0456, -0.0327,  0.0391,\n",
      "         0.0301, -0.0563,  0.0332,  0.0598,  0.0167,  0.0432, -0.0211, -0.0159,\n",
      "         0.0703, -0.0087,  0.0291, -0.0279,  0.0589,  0.0299,  0.0591, -0.0355,\n",
      "         0.0441, -0.0019,  0.0481, -0.0165,  0.0686,  0.0002, -0.0038, -0.0415,\n",
      "        -0.0462, -0.0092,  0.0681, -0.0355, -0.0459,  0.0347,  0.0144,  0.0113,\n",
      "        -0.0236, -0.0265,  0.0087,  0.0359,  0.0273, -0.0367,  0.0276,  0.0137,\n",
      "         0.0024, -0.0417, -0.0256,  0.0269, -0.0084,  0.0582,  0.0537, -0.0043,\n",
      "        -0.0254,  0.0277,  0.0546, -0.0068,  0.0217,  0.0124, -0.0385,  0.0009,\n",
      "        -0.0580,  0.0238,  0.0073,  0.0236,  0.0368,  0.0671, -0.0637, -0.0185,\n",
      "         0.0597, -0.0361, -0.0569,  0.0683,  0.0470, -0.0044, -0.0160, -0.0300,\n",
      "         0.0580,  0.0548,  0.0470, -0.0231, -0.0522,  0.0140, -0.0558,  0.0704,\n",
      "         0.0576,  0.0116, -0.0646,  0.0248,  0.0553,  0.0213,  0.0295, -0.0494,\n",
      "         0.0003, -0.0517, -0.0085,  0.0034,  0.0643, -0.0570,  0.0543,  0.0513,\n",
      "        -0.0494,  0.0032,  0.0405, -0.0591, -0.0341,  0.0441, -0.0096, -0.0097,\n",
      "        -0.0108,  0.0633, -0.0090,  0.0606,  0.0497, -0.0182, -0.0131,  0.0436,\n",
      "         0.0384, -0.0602,  0.0106, -0.0123, -0.0450, -0.0368,  0.0219, -0.0129,\n",
      "         0.0174, -0.0265, -0.0194,  0.0228, -0.0373, -0.0081,  0.0232,  0.0037,\n",
      "         0.0201, -0.0078, -0.0254,  0.0167, -0.0375, -0.0448,  0.0180,  0.0523,\n",
      "         0.0341,  0.0654,  0.0093, -0.0548,  0.0292, -0.0138,  0.0584, -0.0104,\n",
      "         0.0301,  0.0180,  0.0066, -0.0210,  0.0233,  0.0289, -0.0681, -0.0235,\n",
      "         0.0433, -0.0470, -0.0425,  0.0200, -0.0078, -0.0612, -0.0657, -0.0663,\n",
      "        -0.0189,  0.0093, -0.0567,  0.0034,  0.0500,  0.0086, -0.0692,  0.0662,\n",
      "        -0.0050, -0.0499, -0.0112, -0.0022,  0.0551,  0.0551,  0.0019,  0.0579,\n",
      "        -0.0625, -0.0526,  0.0153, -0.0377, -0.0288, -0.0325, -0.0462,  0.0485,\n",
      "        -0.0440, -0.0339,  0.0689, -0.0555,  0.0082, -0.0347,  0.0630,  0.0446,\n",
      "         0.0415, -0.0236, -0.0327, -0.0199, -0.0196,  0.0351,  0.0700,  0.0032,\n",
      "        -0.0027, -0.0214, -0.0469,  0.0379, -0.0028,  0.0078, -0.0078, -0.0116,\n",
      "         0.0463,  0.0107,  0.0445, -0.0503,  0.0257,  0.0323,  0.0114,  0.0502,\n",
      "        -0.0674,  0.0129, -0.0101, -0.0479, -0.0173, -0.0149,  0.0628, -0.0609,\n",
      "        -0.0252,  0.0357,  0.0324,  0.0456, -0.0274, -0.0615,  0.0434,  0.0664,\n",
      "         0.0506, -0.0620, -0.0139,  0.0195, -0.0200,  0.0223, -0.0383,  0.0123,\n",
      "         0.0161,  0.0107,  0.0651, -0.0065, -0.0124,  0.0573,  0.0021,  0.0033,\n",
      "        -0.0636,  0.0225,  0.0080, -0.0152,  0.0020, -0.0516,  0.0202, -0.0383,\n",
      "        -0.0682, -0.0446,  0.0139,  0.0602,  0.0275, -0.0385,  0.0158,  0.0522,\n",
      "        -0.0654, -0.0405,  0.0110, -0.0304, -0.0372,  0.0178,  0.0104,  0.0399,\n",
      "        -0.0009,  0.0449, -0.0673, -0.0018,  0.0188,  0.0462, -0.0296, -0.0431,\n",
      "         0.0334, -0.0631,  0.0644, -0.0012,  0.0379, -0.0705, -0.0167, -0.0578,\n",
      "         0.0596,  0.0615, -0.0313,  0.0440, -0.0369,  0.0504,  0.0317, -0.0517,\n",
      "         0.0465,  0.0392, -0.0038,  0.0601, -0.0238,  0.0037, -0.0423,  0.0560,\n",
      "         0.0608,  0.0161,  0.0433, -0.0027,  0.0350, -0.0599, -0.0672, -0.0590,\n",
      "        -0.0259,  0.0247, -0.0615, -0.0331, -0.0232,  0.0582, -0.0273,  0.0232,\n",
      "        -0.0425,  0.0706, -0.0118, -0.0465, -0.0057, -0.0568,  0.0559, -0.0110,\n",
      "         0.0039,  0.0676,  0.0540,  0.0627,  0.0178, -0.0462, -0.0704,  0.0646,\n",
      "         0.0665,  0.0636, -0.0172,  0.0214, -0.0129, -0.0038, -0.0138,  0.0658,\n",
      "         0.0056,  0.0218,  0.0293, -0.0092,  0.0065, -0.0523,  0.0544, -0.0142,\n",
      "        -0.0237,  0.0015,  0.0526, -0.0355, -0.0541,  0.0467, -0.0638,  0.0537,\n",
      "        -0.0326,  0.0407,  0.0091,  0.0117,  0.0668, -0.0509, -0.0233,  0.0567,\n",
      "        -0.0290,  0.0207, -0.0001,  0.0447,  0.0522, -0.0075,  0.0691,  0.0086,\n",
      "         0.0267,  0.0058,  0.0700, -0.0161,  0.0275, -0.0431,  0.0022, -0.0019,\n",
      "        -0.0142, -0.0592, -0.0211, -0.0300,  0.0556,  0.0360,  0.0661, -0.0051,\n",
      "        -0.0265,  0.0119,  0.0496,  0.0281,  0.0430, -0.0293,  0.0453, -0.0677,\n",
      "         0.0532,  0.0275,  0.0242,  0.0325, -0.0574,  0.0152,  0.0057,  0.0157,\n",
      "        -0.0074, -0.0694,  0.0608,  0.0698, -0.0514, -0.0707, -0.0577, -0.0195],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0209,  0.0150, -0.0527,  ...,  0.0676, -0.0837, -0.0311],\n",
      "        [-0.0533, -0.0632, -0.0382,  ...,  0.0970, -0.0794,  0.0291],\n",
      "        [ 0.0599, -0.0277,  0.0859,  ..., -0.0371,  0.0217, -0.0029],\n",
      "        ...,\n",
      "        [-0.0633, -0.0043, -0.0036,  ...,  0.0501,  0.0383,  0.0202],\n",
      "        [-0.0730, -0.0104,  0.0778,  ..., -0.0684,  0.0168,  0.0198],\n",
      "        [-0.0599, -0.0843,  0.0099,  ..., -0.0627, -0.0341,  0.0918]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0.,  ..., 0., 0., 0.], device='cuda:0', requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
